%% LyX 2.3.6 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{url}
\usepackage{esint}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

\makeatother

\usepackage{babel}
\begin{document}
\title{Solving differential equations with global optimization techniques}
\author{Ioannis G. Tsoulos$^{(1)}$\thanks{Corresponding author. Email: itsoulos@uoi.gr},
Alexandros Tzallas$^{(1)}$, Dimitrios Tsalikakis$^{(2)}$}
\date{$^{(1)}$Department of Informatics and Telecommunications, University
of Ioannina, 47100 Arta, Greece \\
$^{(2)}$University of Western Macedonia, Department of Engineering
Informatics and Telecommunications, Greece}
\maketitle
\begin{abstract}
The solution of differential equations finds many applications in
a huge range of problems, and many techniques have been developed
to approximate their solutions. This manuscript presents a number
of global optimization techniques that have been successfully applied
to train machine learning models to approximate differential equation
solutions. These methods have been successfully applied to solving
ordinary differential equations and systems of differential equations
as well as partial differential equations with Dirichlet boundary
conditions.
\end{abstract}
\textbf{Keywords}: Differential equations; global optimization; stochastic
methods; machine learning

\section{Introduction}

The solution ordinary differential equations (ODEs), systems of differential
equations (SODEs) and partial differential equations (PDEs) is commonly
used in many scientific fields, such as physics \cite{de_physics1,de_physics2},
chemistry \cite{de_chem1,de_chem2,de_chem3}, economics \cite{de_econ1,de_econ2},
biology \cite{de_bio1,de_bio2} etc. It is obvious that the numerical
solution of differential equations has positive effects in many scientific
areas and, for this reason, many techniques have been proposed in
the modern literature. These methods can include the well - known
Runge Kutta methods \cite{de_rk1,de_rk2,de_rk3}, wavelet transformations
\cite{de_wave1,de_wave2,de_wave3}, predictor - corrector variations
\cite{de_pred1,de_pred2}, methods that incorporate artificial neural
networks \cite{neural1,neural2} to solve differential equations \cite{de_nn1,de_nn2,de_nn3},
finite element methods \cite{de_finite1,de_finite2} etc. Also, recently
a novel method based on Grammatical Evolution \cite{ge_main} has
been proposed to tackle differential equations in closed analytical
form by Tsoulos and Lagaris \cite{de_tsoulos}. 

In this work, the differential equations to be solved are presented
as machine learning models and the solution of the corresponding equation
is reduced to a global optimization problem, where the task is to
locate the global minimum of a multidimensional function $f(x):\ S\subset R^{n}\rightarrow R$
with $x_{i}\ \in\left[L_{i},R_{i}\right],\ i=1..n$. The value of
$n$ is the number of parameters in the corresponding machine learning
model. For example, the number of weights and biases in some artificial
neural network. The vector $L$ is considered as the lower bound of
the parameter $x$, and and the vector $R$ as the upper bound. Machine
learning models have been used a lot in solving differential equations,
for example the use of SVM techniques \cite{de_svm1,de_svm2}, the
incorporation of deep learning methods \cite{de_deep1,de_deep2},
constructed neural networks \cite{de_nnc} etc. In current work, two
machine learning models were tested: artificial neural networks and
Radial Basis Function networks (RBF) \cite{RbfMain}. The parameters
in these machine learning models were trained using modified versions
of two well-known global optimization techniques: Genetic Algorithms
\cite{ga1,ga2,ga3} and the PSO method \cite{pso1,pso2}. These models
have been successfully used to solve ordinary differential equations,
systems of differential equations as well as partial differential
equations with Dirichlet boundary conditions.

The rest of this article is organized as follows: in section \ref{sec:Method-description}
the used models and the proposed modified global optimization techniques
are outlined in detail, in section \ref{sec:Experiments} the differential
equations used in the experiments are listed as well as the experimental
results and finally in section \ref{sec:Conclusions} some conclusions
about the used methods are presented.

\section{Method description \label{sec:Method-description}}

\subsection{The neural network model }

Artificial neural networks are parametric mathematical models used
in many scientific areas with great importance, such as physics \cite{nnphysics1,nnphysics2,nnphysics3},
chemistry \cite{nnchem1,nnchem2,nnchem3}, medicine \cite{nnmed1,nnmed2}
etc. A neural network can be modeled as a function $N(\overrightarrow{x},\overrightarrow{w})$,
where the vector $\overrightarrow{x}$ is called the input vector
or pattern and $\overrightarrow{w}$ is called the weight vector.
A method that trains a neural network should be used to estimate the
vector $\overrightarrow{w}$ for a certain problem. The optimization
technique minimizes the following quantity:
\begin{equation}
E\left(N\left(\overrightarrow{x},\overrightarrow{w}\right)\right)=\sum_{i=1}^{M}\left(N\left(\overrightarrow{x}_{i},\overrightarrow{w}\right)-y_{i}\right)^{2}\label{eq:eq1}
\end{equation}
In the equation \ref{eq:eq1} (commonly addressed also as error function),
the value $y_{i}$ denotes the actual output for the point $\overrightarrow{x_{i}}$
. The form for the neural network is the same as in the case of \cite{nnc}.
Let us have a neural network with one processing layer and the output
of each processing unit is given by:
\begin{equation}
o_{i}(x)=\sigma\left(p_{i}^{T}x+\theta_{i}\right),\label{eq:eq1-1}
\end{equation}
with $p_{i}$ the weight vector and $\theta_{i}$ is the bias for
the processing unit $i$. The function $\sigma(x)$ is the well -
known sigmoid function and it is given by: 
\begin{equation}
\sigma(x)=\frac{1}{1+\exp(-x)}\label{eq:sig}
\end{equation}
If the neural network has $H$ hidden nodes, then the output of the
entire network is given by:
\begin{equation}
N(x)=\sum_{i=1}^{H}v_{i}o_{i}(x),\label{eq:eq1-2}
\end{equation}
with $v_{i}$ being the output weight for the processing node $i$.
Therefore, if a single vector is used for both weights and biases,
we will have the following form for the artificial neural network:
\begin{equation}
N\left(\overrightarrow{x},\overrightarrow{w}\right)=\sum_{i=1}^{H}w_{(d+2)i-(d+1)}\sigma\left(\sum_{j=1}^{d}x_{j}w_{(d+2)i-(d+1)+j}+w_{(d+2)i}\right)\label{eq:nn}
\end{equation}
where $d$ is the dimension of vector $\overrightarrow{x}$. 

\subsection{The RBF model}

An RBF network has also a lot of applications \cite{rbfphysics1,rbfchemistry1,rbfmed1}
and it is denoted as a function $r(x)$:\textbf{
\begin{equation}
r(x)=\sum_{i=1}^{H}w_{i}\phi\left(\left\Vert x-c_{i}\right\Vert \right)\label{eq:firstrbf}
\end{equation}
}with \textbf{$\overrightarrow{x}$ }being the input vector and the
vector\textbf{ $\overrightarrow{w}$ }is considered as the weight
vector. The function $\phi(x)$ used here is the following Gaussian
function:

\textbf{
\begin{equation}
\phi(x)=\exp\left(-\frac{\left(x-c\right)^{2}}{\sigma^{2}}\right)
\end{equation}
}The function $\phi(x)$ has the property that its valued depends
on the distance between the vectors\textbf{ $\overrightarrow{x},\ $$\overrightarrow{c}$}. 

\subsection{Calculation of error\label{subsec:Calculation-of-error}}

This subsection details the calculation of the error of the machine
learning models for each differential equation. In each equation case
the initial or boundary conditions are imposed using penalty factors.

\subsubsection{Calculation for ODEs }

The ODEs are defined as: 
\begin{equation}
\psi\left(x,y,y^{(1)},\ldots,y^{(n)}\right)=0,\ x\in[a,b]
\end{equation}
with $y^{(i)}$ the ith-order derivative of $y(x).$ The corresponding
conditions are: 
\begin{equation}
h_{i}\left(x,y,y^{(1)},\ldots,y^{(n)}\right)_{|x=t_{i}},i=1,\ldots,n
\end{equation}
where $t_{i}$ is either $a$ or $b$. The calculation of the model
error $f(r)$ of a model $r$ are the following:
\begin{enumerate}
\item \textbf{Create} $T=\left\{ x_{1}=a,x_{2},x_{3},\ldots,x_{N}=b\right\} $
a set of equidistant points.
\item \textbf{Calculate} the error value $E_{r}=\sum_{i=1}^{N}\psi\left(x_{i},r\left(x_{i}\right),r^{(1)}\left(x_{i}\right),\ldots,r^{(n)}\left(x_{i}\right)\right)^{2}$
\item \textbf{Calculate} the penalty factor for the initial conditions:
\begin{equation}
P_{r}=\lambda\sum_{k=1}^{n}h_{k}^{2}\left(x,r(x),r^{(1)}(x),\ldots,r^{(n)}(x)\right)_{|x=t_{k}}
\end{equation}
, where $\lambda>0$.
\item \textbf{Return} $f(r)=E_{r}+P_{r}$
\end{enumerate}

\subsubsection{Calculation for SODEs }

The system of odes used in the current work is in in the form:
\begin{equation}
\left(\begin{array}{ccc}
\psi_{1}\left(x,y_{1},y_{1}^{(1)},y_{2},y_{2}^{(1)},\ldots,y_{k},y_{k}^{(1)}\right) & = & 0\\
\psi_{2}\left(x,y_{1},y_{1}^{(1)},y_{2},y_{2}^{(1)},\ldots,y_{k},y_{k}^{(1)}\right) & = & 0\\
\vdots & \vdots & \vdots\\
\psi_{k}\left(x,y_{1},y_{1}^{(1)},y_{2},y_{2}^{(1)},\ldots,y_{k},y_{k}^{(1)}\right) & = & 0
\end{array}\right)
\end{equation}
with $x\in[a,b]$. The initial conditions of the system are defined
as the following vector: 
\begin{equation}
\left(\begin{array}{ccc}
y_{1}(a) & = & y_{1a}\\
y_{2}(a) & = & y_{2a}\\
\vdots & \vdots & \vdots\\
y_{k}(a) & = & y_{ka}
\end{array}\right)\label{eq:eqinit}
\end{equation}
The error value for a set of models $r_{1},r_{2},...,r_{k}$ is calculated
using the following:
\begin{enumerate}
\item \textbf{Create} $T=\left\{ x_{1}=a,x_{2},x_{3},\ldots,x_{N}=b\right\} $
a set of equidistant points.
\item \textbf{For every model $r_{i},\ i=1,..k$ do}
\begin{enumerate}
\item \textbf{Calculate} the error value: $E_{r_{i}}=\sum_{j=1}^{N}\left(\psi_{i}\left(x_{j},r_{1},r_{1}^{(1)},r_{2},r_{2}^{(1)},\ldots,r_{k},r_{k}^{(1)}\right)\right)^{2}$
\item \textbf{Calculate} the corresponding penalty value: $P_{r_{i}}=\lambda\left(r_{i}\left(a\right)-y_{ia}\right)^{2}$
\end{enumerate}
\item \textbf{EndFor}
\item \textbf{Calculate} the total error value: $f(r)=\sum_{i=1}^{k}\left(E_{r_{i}}+P_{r_{i}}\right)$
\end{enumerate}

\subsubsection{Calculation for PDEs}

The partial differential equations solved and test in the current
work have the following form:
\begin{equation}
h\left(x,y,\Psi(x,y),\frac{\partial}{\partial x}\Psi(x,y),\frac{\partial}{\partial y}\Psi(x,y),\frac{\partial^{2}}{\partial x^{2}}\Psi(x,y),\frac{\partial^{2}}{\partial y^{2}}\Psi(x,y)\right)=0
\end{equation}
with $x\in[a,b],\ y\in[c,d]$. The boundary conditions consider in
this manuscript are the Dirichlet boundary conditions with the following
form:
\begin{enumerate}
\item $\Psi(a,y)=f_{0}(y)$
\item $\Psi(b,y)=f_{1}(y)$
\item $\Psi(x,c)=g_{0}(x)$
\item $\Psi(x,d)=g_{1}(x)$
\end{enumerate}
The steps to calculate the fitness $f(g)$ for any given chromosome
are the following:
\begin{enumerate}
\item \textbf{Define} the set $T=\left\{ \left(x_{1},y_{1}\right),\left(x_{1},y_{2}\right),\ldots,\left(x_{N},y_{N}\right)\right\} $
of $N\times N$ points in $[a,b]\times[c,d]$.
\item \textbf{Define} the set $x_{B}=\left\{ x_{b1},x_{b2},\ldots,x_{bN}\right\} $
equidistant points in $[a,b]$.
\item \textbf{Define} the set $y_{B}=\left\{ y_{b1},y_{b2},\ldots,y_{bN}\right\} $equidistant
points in $[c,d]$.
\item \textbf{Calculate} the error value $E_{r}$ as 
\[
E_{r}=\sum_{i=1}^{N}h\left(x_{i},y_{i},r\left(x_{i},y_{i}\right),\frac{\partial}{\partial x}r\left(x_{i},y_{i}\right),\frac{\partial}{\partial y}r\left(x_{i},y_{i}\right)\right)^{2}
\]
\item \textbf{Calculate} the associated penalty values:
\[
\begin{array}{ccc}
P_{1r} & = & \lambda\sum_{i=1}^{M}\left(r\left(a,y_{bi}\right)-f_{0}\left(y_{bi}\right)\right)^{2}\\
P_{2r} & = & \lambda\sum_{i=1}^{M}\left(r\left(b,y_{bi}\right)-f_{1}\left(y_{bi}\right)\right)^{2}\\
P_{3r} & = & \lambda\sum_{i=1}^{M}\left(r\left(x_{bi},c\right)-g_{0}\left(x_{bi}\right)\right)^{2}\\
P_{4r} & = & \lambda\sum_{i=1}^{M}\left(r\left(x_{bi},d\right)-g_{1}\left(x_{bi}\right)\right)^{2}
\end{array}
\]
\item \textbf{Calculate} the total fitness as $f(r)=E_{r}+P_{1r}+P_{2r}+P_{3r}+P_{4r}$
\end{enumerate}

\subsection{The used genetic algorithm }

The Genetic Algorithm is a global optimization technique inspired
by biology and it includes a series of genetic operations such as
selection, crossover and mutation. This method initiates by creating
a population of candidate solutions, called also chromosomes, and
subsequently these solutions are evolved through the application of
genetic operations. The method has been applied with success in a
variety of research problems such as electromagnetic \cite{gen_app1},
combinatorial problems \cite{gen_app2},\textbf{ }design of water
distribution networks \cite{gen_app3} etc. In this work, a hybrid
form of genetic algorithm is presented, where a local minimization
method is periodically applied to a subset of the genetic algorithm's
chromosomes. Furthermore, the termination rule has taken into account
the nature of the problem.

The main steps of the used genetic algorithm are as follows:
\begin{enumerate}
\item \textbf{Initialization} step.
\begin{enumerate}
\item \textbf{Set} as $N_{c}$ the number of chromosomes that will participate.
\item \textbf{Set} as $N_{g},$the maximum number of allowed generations.
\item \textbf{Set} as $p_{m}$, the mutation rate.
\item \textbf{Set} as $p_{s},$ the selection rate.
\item \textbf{Set} as $p_{l},$ the local search rate.
\item \textbf{Set} $\epsilon$ a small positive number, i.e $\epsilon=10^{-8}$.
\item Initialize randomly the chromosomes $x_{i},\ i=1,...,N_{c}$
\item \textbf{Set} iter=0
\end{enumerate}
\item \textbf{Check} for termination\label{enu:Check-for-termination.}.
\begin{enumerate}
\item \textbf{Obtain} the best fitness 
\[
f^{*}=\min_{i\in\left[0\ldots N_{c}\right]}f_{i}
\]
\item \textbf{Terminate} if $\mbox{iter}\ge N_{g}$ OR $f^{*}\le\epsilon$
\end{enumerate}
\item \textbf{Calculate} fitness. 
\begin{enumerate}
\item \textbf{For} $i=1,\ldots,N_{c}$ \textbf{do}
\begin{enumerate}
\item \textbf{Create} a neural network or a RBF network using as parameters
the chromosome $x_{i}$ and denote this model as $m_{i}$. For the
case of SODEs, the chromosome is divided into $k$ parts (the number
of ODEs in the system) and a model $m_{ij},\ j=1,..,k$ is constructed
for every equation.
\item \textbf{Calculate} the fitness value $f_{i}=f\left(m_{i}\right)$
using the error equations of subsection \ref{subsec:Calculation-of-error}.
\item \textbf{If} $r\le p_{l}$, where $r\in[0,1]$ a random number, apply
a local search procedure to the model $m_{i}$ and locate a better
value $f_{i}$ for fitness. The used local search procedure is a BFGS
method \cite{powell}.
\end{enumerate}
\item \textbf{EndFor}
\end{enumerate}
\item \textbf{Application} of genetic operators.
\begin{enumerate}
\item \textbf{Selection} operation. During selection, the chromosomes are
classified according to their fitness. The first\textbf{ $\left(1-p_{s}\right)\times N_{c}$}
are copied without changes to the next generation of the population.
The rest will be replaced by chromosomes that will be produced at
the crossover.
\item \textbf{Crossover} operation. In the crossover operation, $p_{s}\times N_{c}$
chromosomes are produced. For every couple of produced offsrpings,
two parents $(z,w)$ are selected using the well - known procedure
of tournament selection.\textbf{ }For every pair $(z,w)$ of parents,
two offsprings $\tilde{z}$ and $\tilde{w}$ are produced according
to the following equations:\textbf{
\begin{eqnarray}
\tilde{z_{i}} & = & a_{i}z_{i}+\left(1-a_{i}\right)w_{i}\nonumber \\
\tilde{w_{i}} & = & a_{i}w_{i}+\left(1-a_{i}\right)z_{i}\label{eq:crossover_ali-1}
\end{eqnarray}
}where $a_{i}$ is a random number with the property $a_{i}\in[-0.5,1.5]$
\cite{kaeloAli}. 
\item \textbf{Mutation} operation. For each element of every chromosome
a random number $r\in\left[0,1\right]$ is produced. Subsequently
we change randomly the corresponding element if $r\le p_{m}$
\item \textbf{Set} iter=iter+1
\end{enumerate}
\item \textbf{Goto} step \ref{enu:Check-for-termination.}.
\end{enumerate}

\subsection{The used PSO method }

The method PSO is a stochastic global optimization method and is based
on a population of candidate solutions (particles) that move to search
for the global minimum at some speed that is constantly changing.
The speed of each solution is affected by the best position in which
the specific particle has been found so far, but also by the overall
best position of the swarm of particles.The PSO method has been applied
on a wide range of applications \cite{pso_app1,pso_app2,pso_app3,pso_app4}.
This manuscript proposes a hybrid PSO method with a periodical application
of a local search procedure. The steps of the modified PSO method
are the following:
\begin{enumerate}
\item \textbf{Initialization}. 

\begin{enumerate}
\item \textbf{Set} $\mbox{iter}=0$.
\item \textbf{Set} as $N_{c}$ the number of particles. 
\item \textbf{Set} as $N_{g}$ the maximum number of generations.
\item \textbf{Set} $p_{l}$ the local search rate.
\item \textbf{Set} $\epsilon$ a small positive number, i.e $\epsilon=10^{-8}$.
\item \textbf{Randomly} initialize the positions of the particles $x_{1},x_{2},...,x_{N_{c}}$.
The size of each particle is defined as $n$.
\item \textbf{Randomly} initialize the velocities of the particles $u_{1},u_{2},...,u_{N_{c}}$
using the scheme
\[
u_{ij}=L_{j}+r\times\frac{R_{j}-L_{j}}{20},\ i=1,...,N_{c},\ j=1,..,n
\]
where $r$ is a random number with $r\in[0,1]$, $R_{j}$ is the lower
bound for parameter $j$ and $R_{j}$ is the upper bound for parameter
$j$.
\item \textbf{For} $i=1..N_{c}$ do $p_{i}=x_{i}$, where $p_{i}$ is the
best located position for particle $i$.
\item \textbf{Set} $p_{\mbox{best}}=\arg\min_{i\in1..N_{c}}f\left(x_{i}\right)$
\end{enumerate}
\item \textbf{Termination Check}.\label{enu:Check-Termination.} 
\begin{enumerate}
\item \textbf{Obtain} the best fitness 
\[
f^{*}=\min_{i\in\left[0\ldots N_{c}\right]}f_{i}
\]
\item \textbf{Terminate} if $\mbox{iter}\ge N_{g}$ OR $f^{*}\le\epsilon$
\end{enumerate}
\item \textbf{For} $i=1..N_{c}$ \textbf{Do}

\begin{enumerate}
\item Update the velocity $u_{i}$ 
\[
u_{ij}=u_{ij}+r_{1}\times\left(p_{ij}-x_{ij}\right)+r_{2}\times\left(p_{\mbox{best},j}-x_{ij}\right),\ j=1,..,n
\]
\item Update the position $x_{i}$ as $x_{i}=x_{i}+u_{i}$
\item Evaluate the fitness of the particle $x_{i}$ using the same scheme
as for the genetic algorithm. 
\item \textbf{If} $r\le p_{l}$, where $r\in[0,1]$ a random number, apply
a local search procedure to the model $m_{i}$ and locate a better
value $f_{i}$ for the fitness of particle $x_{i}$.
\item If $f\left(x_{i}\right)\le f\left(p_{i}\right)$ then $p_{i}=x_{i}$
\end{enumerate}
\item \textbf{End} \textbf{For}
\item \textbf{Set} $p_{\mbox{best}}=\arg\min_{i\in1..m}f\left(x_{i}\right)$
\item \textbf{Set} $\mbox{iter}=\mbox{iter}+1$. 
\item \textbf{Goto} Step \ref{enu:Check-Termination.}
\end{enumerate}

\section{Experiments \label{sec:Experiments}}

The differential equations used in the experiments are presented below.
These equations have been used in experiments and other related publications\cite{de_nn1,de_tsoulos}.

\subsection{Linear ode equations}

\subsubsection*{ODE1}

\[
y'=\frac{2x-y}{x}
\]
with initial condition $y(1)=3,\ x\in[1,2]$. The exact solution is
$y(x)=x+\frac{2}{x}$

\subsubsection*{ODE2}

\[
y'=\frac{1-y\cos(x)}{\sin(x)}
\]
with $y(1)=\frac{3}{\sin(1)},\ x\in[1,2]$. The solution is given
by $y(x)=\frac{x+2}{\sin(x)}$

\subsubsection*{ODE3}

\[
y''=6y'-9y
\]
The initial values are $y(0)=0,\ y'(0)=2,\ x\in[0,1]$ and the solution
is $y(x)=2x\exp(3x)$

\subsubsection*{ODE4}

\[
y''=-\frac{1}{5}y'-y-\frac{1}{5}\exp\left(-\frac{x}{5}\right)\cos(x)
\]
where $y(0)=0,\ y(1)=\frac{\sin(0.1)}{\exp(0.2)},\ x\in[0,1]$ and
the exact solution is $y(x)=\exp\left(-\frac{x}{5}\right)\sin(x)$

\subsubsection*{ODE5}

\[
y''=-100y
\]
where $y(0)=0,\ y'(0)=10,\ x\in[0,1]$ and the solution is given by
\[
y(x)=\sin(10x)
\]


\subsection{Non-linear ODEs}

\subsubsection*{NLODE1 }

\[
y'=\frac{1}{2y}
\]
with $y(1)=1,\ x\in[1,4].$ The solution is given by $y(x)=\sqrt{x}$

\subsubsection*{NLODE2 }

\[
(y')^{2}+\log(y)-\cos^{2}(x)-2\cos(x)-1-\log(x+\sin(x))=0
\]
with $y(1)=1+\sin(1),\ x\in[1,2]$. The solution is given by $y(x)=x+\sin(x)$

\subsubsection*{NLODE3 }

\[
y''y'=-\frac{4}{x^{3}}
\]
with $y(1)=0,\ y(2)=\log(4),\ x\in[1,2]$ and the analytical solution
is $y(x)=\log\left(x^{2}\right)$

\subsubsection*{NLODE4 }

\[
x^{2}y''+\left(xy'\right)^{2}+\frac{1}{\log(x)}=0
\]
with $y(e)=0,\ y'(e)=\frac{1}{e},\ x\in[e,2e]$ and solution $y(x)=\log(\log(x))$

\subsection{Odes without analytic solution}

\subsubsection*{UNSOLODE1}

\[
xy''+y'-\cos(x)=0
\]
with $y(0)=0,\ y'(0)=1,\ x\in[0,1]$. The exact solution is given
by
\[
y(x)=\int_{0}^{x}\frac{\sin(t)}{t}dt
\]


\subsubsection*{UNSOLODE2}

\[
y''+2xy=0
\]
with $y(0)=0,\ y'(0)=1,\ x\in[0,1]$. The exact solution is given
by
\[
y(x)=\int_{0}^{x}\exp\left(-t^{2}\right)dt
\]


\subsection{Systems of ode cases }

\subsubsection*{SODE1 }

\begin{eqnarray*}
y'_{1} & = & \cos(x)+y_{1}^{2}+y_{2}-\left(x^{2}+\sin^{2}(x)\right)\\
y'_{2} & = & 2x-x^{2}\sin(x)+y_{1}y_{2}
\end{eqnarray*}
with $y_{1}(0)=0,\ y_{2}(0)=0,\ x\in[0,1]$. The analytical solutions
are given by $y_{1}(x)=\sin(x),\ y_{2}(x)=x^{2}$.

\subsubsection*{SODE2 }

\begin{eqnarray*}
y'_{1} & = & \frac{\cos(x)-\sin(x)}{y_{2}}\\
y'_{2} & = & y_{1}y_{2}+\exp(x)-\sin(x)
\end{eqnarray*}
where $y_{1}(0)=0,\ y_{2}(0)=1,\ x\in[0,1]$ and the solutions are
$y_{1}(x)=\frac{\sin(x)}{\exp(x)},\ y_{2}=\exp(x)$

\subsubsection*{SODE3 }

\begin{eqnarray*}
y'_{1} & = & \cos(x)\\
y'_{2} & = & -y_{1}\\
y'_{3} & = & y_{2}\\
y'_{4} & = & -y_{3}\\
y'_{5} & = & y_{4}
\end{eqnarray*}
with $y_{1}(0)=0,\ y_{2}(0)=1,\ y_{3}(0)=0,\ y_{4}(0)=1,\ y_{5}(0)=0,\ x\in[0,1]$
and the solutions $y_{1}(x)=\sin(x),\ y_{2}(x)=\cos(x),\ y_{3}(x)=\sin(x),\ y_{4}(x)=\cos(x),\ y_{5}(x)=\sin(x)$.

\subsubsection*{SODE4}

\begin{eqnarray*}
y'_{1} & = & -\frac{1}{y_{2}}\sin\left(\exp(x)\right)\\
y'_{2} & = & -y_{2}
\end{eqnarray*}
with $y_{1}(0)=\cos(1.0),\ y_{2}(0)=1.0,\ x\in[0,1]$ and solutions
$y_{1}(x)=\cos\left(\exp(x)\right),\ y_{2}(x)=\exp(-x)$.

\subsection{Pde cases}

\subsubsection*{PDE1 }

\[
\nabla^{2}\Psi(x,y)=\exp(-x)\left(x-2+y^{3}+6y\right)
\]
with $x\in[0,1],\ y\in[0,1]$ and boundary conditions: $\Psi(0,y)=y^{3},\ \Psi(1,y)=\left(1+y^{3}\right)\exp(-1),\ \Psi(x,0)=x\exp(-x),\ \Psi(x,1)=(x+1)\exp(-x)$
The solution is: $\Psi(x,y)=\left(x+y^{3}\right)\exp(-x)$

\subsubsection*{PDE2 }

\[
\nabla^{2}\Psi(x,y)=-2\Psi(x,y)
\]
with $x\in[0,1],\ y\in[0,1]$ and boundary conditions: $\Psi(0,y)=0,\ \Psi(1,y)=\sin(1)\cos(y),\ \Psi(x,0)=\sin(x),\ \Psi(x,1)=\sin(x)\cos(1)$.
The solution is $\Psi(x,y)=\sin(x)\cos(y)$. 

\subsubsection*{PDE3 }

\[
\nabla^{2}\Psi(x,y)=4
\]
 with $x\in[0,1],\ y\in[0,1]$ and boundary conditions: $\Psi(0,y)=y^{2}+y+1,\ \Psi(1,y)=y^{2}+y+3,\ \Psi(x,0)=x^{2}+x+1,\ \Psi(x,1)=x^{2}+x+3$.
The solution is: $\Psi(x,y)=x^{2}+y^{2}+x+y+1$.

\subsubsection*{PDE4}

\[
\nabla^{2}\Psi(x,y)=\left(x-2\right)\exp(-x)+x\exp(-y)
\]
with $x\in[0,1],\ y\in[0,1]$ and boundary conditions: $\Psi(0,y)=0,\ \Psi(1,y)=\sin(y),\ \Psi(x,0)=0,\ \Psi(x,1)=\sin(x)$.
The solution is: $\Psi(x,y)=\sin(xy)$.

\subsection{Experimental results}

A series of experiments were conducted in order to determine the efficiency
and the validity of the proposed methods. The equations as well as
the proposed methods were coded in ANSI C++ using the OPTIMUS optimization
library, freely available from \url{https://github.com/itsoulos/OPTIMUS/}.
The values for the parameters of the methods are listed in the table
\ref{tab:Parameter-settings.}. The weights for the neural network
as well as for the RBF case were set to 10 and all the experiments
were conducted 30 times and averages were reported. In every independent
run, a  different seed for the random generator was used. In our case,
the drand48() random generator of C++ was utilized. The solution produced
by the neural network was applied to random points within the equation
domain, and the error at those points was recorded. We denote this
error as generalization error. The results are listed in the table
\ref{tab:genetic_results}. The columns have the following meaning:
\begin{enumerate}
\item The column EQUATION denotes the title of the equation to be solved.
\item The column MLP-GEN denotes the application of a neural network with
ten hidden nodes and trained using the proposed genetic algorithm.
\item The column RBF-GEN stands for the application of an RBF model with
10 processing units. The weights as well as the centroids of the RBF
network were trained using the proposed genetic algorithm.
\item The column MLP-PSO stands for the application of a neural network
with 10 hidden nodes that was adapted to the given equation using
the proposed PSO algorithm.
\item The column MLP-RBF denotes the application of an RBF to the equation
and the network was trained using the proposed PSO variant.
\end{enumerate}
As far as can be seen from the experimental results, both the artificial
neural network and the RBF network achieve very low errors in almost
all equations. In a small portion of them, the RBF network appears
to have a lower generalization error.

\begin{table}
\caption{Parameter settings for the genetic algorithm.\label{tab:Parameter-settings.}}

\centering{}%
\begin{tabular}{|c|c|}
\hline 
PARAMETER & VALUE\tabularnewline
\hline 
\hline 
$N_{c}$ & 500\tabularnewline
\hline 
$N_{g}$ & 2000\tabularnewline
\hline 
$p_{s}$ & 0.10\tabularnewline
\hline 
$p_{m}$ & 0.05\tabularnewline
\hline 
$p_{l}$ & 0.05\tabularnewline
\hline 
$\lambda$ & 100.0\tabularnewline
\hline 
$N$ & 20\tabularnewline
\hline 
$H$ & 10\tabularnewline
\hline 
\end{tabular}
\end{table}

\begin{table}
\caption{Experimental results using the hybrid genetic algorithm.\label{tab:genetic_results}}

\centering{}%
\begin{tabular}{|c|c|c|c|c|}
\hline 
EQUATION & MLP-GEN & RBF-GEN & MLP-PSO & RBF-PSO\tabularnewline
\hline 
\hline 
ODE1 & $1.6\times10^{-3}$ & $1.7\times10^{-8}$ & $1.6\times10^{-3}$ & $2.1\times10^{-8}$\tabularnewline
\hline 
ODE2 & $1.3\times10^{-3}$ & $1.3\times10^{-7}$ & $2.2\times10^{-3}$ & $2.0\times10^{-8}$\tabularnewline
\hline 
ODE3 & $1.6\times10^{-15}$ & $1.3\times10^{-15}$ & $6.5\times10^{-12}$ & $2.9\times10^{-10}$\tabularnewline
\hline 
ODE4 & $1.8\times10^{-9}$ & $1.7\times10^{-10}$ & $4.4\times10^{-10}$ & $5.7\times10^{-9}$\tabularnewline
\hline 
ODE5 & $3.9\times10^{-5}$ & $7.8\times10^{-2}$ & $3.8\times10^{-10}$ & $5.4\times10^{-2}$\tabularnewline
\hline 
NLODE1 & $1.3\times10^{-15}$ & $7.3\times10^{-10}$ & $1.9\times10^{-9}$ & $7.7\times10^{-9}$\tabularnewline
\hline 
NLODE2 & $2.7\times10^{-8}$ & $1.9\times10^{-10}$ & $7.8\times10^{-12}$ & $2.5\times10^{-9}$\tabularnewline
\hline 
NLODE3 & $3.5\times10^{-6}$ & $1.7\times10^{-8}$ & $1.7\times10^{-8}$ & $3.4\times10^{-8}$\tabularnewline
\hline 
NLODE4 & $4.5\times10^{-8}$ & $4.6\times10^{-9}$ & $3.4\times10^{-8}$ & $4.7\times10^{-8}$\tabularnewline
\hline 
UNSOLODE1 & $2.1\times10^{-10}$ & $5.8\times10^{-11}$ & $3.4\times10^{-12}$ & $1.7\times10^{-8}$\tabularnewline
\hline 
UNSOLODE2 & $1.2\times10^{-15}$ & $1.8\times10^{-8}$ & $1.3\times10^{-12}$ & $5.9\times10^{-8}$\tabularnewline
\hline 
SODE1 & $1.6\times10^{-8}$ & $1.6\times10^{-8}$ & $1.1\times10^{-3}$ & $1.4\times10^{-8}$\tabularnewline
\hline 
SODE2 & $1.3\times10^{-9}$ & $1.8\times10^{-8}$ & $4.1\times10^{-7}$ & $5.3\times10^{-9}$\tabularnewline
\hline 
SODE3 & $1.4\times10^{-9}$ & $6.7\times10^{-9}$ & $1.63\times10^{-9}$ & $7.7\times10^{-9}$\tabularnewline
\hline 
SODE4 & $3.1\times10^{-3}$ & $5.9\times10^{-9}$ & $1.5\times10^{-4}$ & $1.1\times10^{-8}$\tabularnewline
\hline 
PDE1 & $8.1\times10^{-3}$ & $5.5\times10^{-2}$ & $6.7\times10^{-4}$ & $5.9\times10^{-3}$\tabularnewline
\hline 
PDE2 & $7.6\times10^{-5}$ & $9.7\times10^{-3}$ & $3.5\times10^{-6}$ & $4.1\times10^{-4}$\tabularnewline
\hline 
PDE3 & $2.1\times10^{-4}$ & $2.1\times10^{-10}$ & $2.1\times10^{-4}$ & $1.8\times10^{-8}$\tabularnewline
\hline 
PDE4 & $5.6\times10^{-4}$ & $3.6\times10^{-3}$ & $1.7\times10^{-4}$ & $1.9\times10^{-4}$\tabularnewline
\hline 
\end{tabular}
\end{table}


\section{Conclusions \label{sec:Conclusions}}

In this text, the numerical solution of differential equations using
machine learning models was presented. These models were artificial
neural networks and RBF neural networks. The adjustment of the parameters
of these models to the initial conditions of the equations was done
using penalty factors. Finding the optimal parameters for these models
was performed using two modified versions of well - known global optimization
techniques. One global optimization method used was the genetic algorithm
and the other modification was the particle swarm optimization. In
all experimental cases, the generalization error was extremely low
and, indeed in some cases, the RBF network was shown to outperform
the artificial neural network.

Future extensions of the proposed methodology may include:
\begin{enumerate}
\item Incorporation of additional machine learning models, such as SVM models.
\item Usage of more efficient stopping rules. 
\item Parallelization of the whole process using modern programming techniques
such as MPI \cite{openmpi} or OpenMP \cite{openmp}.
\end{enumerate}
\begin{thebibliography}{10}
\bibitem{de_physics1}M. Raissi, G.E. Karniadakis, Hidden physics
models: Machine learning of nonlinear partial differential equations,
Journal of Computational Physics \textbf{357}, pp. 125-141, 2018.

\bibitem{de_physics2}T. Lelièvre, G. Stoltz, Partial differential
equations and stochastic methods in molecular dynamics. Acta Numerica
\textbf{25}, pp. 681-880, 2016.

\bibitem{de_chem1}G. Scholz, F. Scholz, First-order differential
equations in chemistry, ChemTexts \textbf{1}, 2015.

\bibitem{de_chem2}J.L. Padgett, Y. Geldiyev, S. Gautam, W. Peng,
Y. Mechref, A. Ibraguimov, Object classification in analytical chemistry
via data-driven discovery of partial differential equations. Comp
and Math Methods, 2021.

\bibitem{de_chem3}O. Owoyele, P. Pal, ChemNODE: A neural ordinary
differential equations framework for efficient chemical kinetic solvers,
Energy and AI \textbf{7}, 2022.

\bibitem{de_econ1}Z. Wang, X. Huang, H. Shen, Control of an uncertain
fractional order economic system via adaptive sliding mode, Neurocomputing
\textbf{83}, pp. 83-88, 2012.

\bibitem{de_econ2}Y. Achdou, F.J. Buera, J.M. Lasry, P.L. Lions,
B. Moll, Partial differential equation models in macroeconomicsPhil.
Trans. R. Soc. A. \textbf{372}, 2012.

\bibitem{de_bio1}K. Hattaf, N. Yousfi, Global stability for reaction--diffusion
equations in biology, Computers \& Mathematics with Applications \textbf{66},
pp. 1488-1497, 2013.

\bibitem{de_bio2}P. Getto, M. Waurick, A differential equation with
state-dependent delay from cell population biology, Journal of Differential
Equations \textbf{260}, pp. 6176-6200, 2016.

\bibitem{de_rk1}C.A. Kennedy, M.H. Carpenter, Higher-order additive
Runge--Kutta schemes for ordinary differential equations, Applied
Numerical Mathematics \textbf{136}, pp. 183-205, 2019.

\bibitem{de_rk2}H. Ranocha, D.I. Ketcheson, Relaxation Runge--Kutta
Methods for Hamiltonian Problems, J Sci Comput \textbf{84}, 17 2020.

\bibitem{de_rk3}J. Niegemann, R. Diehl, K. Busch, Efficient low-storage
Runge--Kutta schemes with optimized stability regions, Journal of
Computational Physics 231, pp. 364-372, 2012.

\bibitem{de_wave1}Y. Wang. Q. Fan, The second kind Chebyshev wavelet
method for solving fractional differential equations, Applied Mathematics
and Computation \textbf{218}, pp. 8592-8601, 2012.

\bibitem{de_wave2}M.H.Heydari, M.R.Hooshmandasl, F.Mohammadi, Legendre
wavelets method for solving fractional partial differential equations
with Dirichlet boundary conditions, Applied Mathematics and Computation
\textbf{234}, pp. 267-276, 2014.

\bibitem{de_wave3}B. Yuttanan, M. Razzaghi, Legendre wavelets approach
for numerical solutions of distributed order fractional differential
equations, Applied Mathematical Modelling \textbf{70}, pp. 350-364,
2019.

\bibitem{de_pred1}H. Kim, R. Sakthivel, Numerical solution of hybrid
fuzzy differential equations using improved predictor--corrector
method, Communications in Nonlinear Science and Numerical Simulation
\textbf{17}, pp. 3788-3794, 2012.

\bibitem{de_pred2}V. Daftardar-Gejji, Y. Sukale, S. Bhalekar, A new
predictor--corrector method for fractional differential equations,
Applied Mathematics and Computation \textbf{244}, pp. 158-182, 2014.

\bibitem{neural1}C. Bishop, Neural Networks for Pattern Recognition,
Oxford University Press, 1995.

\bibitem{neural2}G. Cybenko, Approximation by superpositions of a
sigmoidal function, Mathematics of Control Signals and Systems 2,
pp. 303-314, 1989.

\bibitem{de_nn1}I. E. Lagaris, A. Likas, D. I. Fotiadis, Artificial
neural networks for solving ordinary and partial differential equations,
IEEE Transactions on Neural Networks \textbf{9}, pp. 987-1000, 1998.

\bibitem{de_nn2}A. Malek, R.Shekari Beidokhti, Numerical solution
for high order differential equations using a hybrid neural network---Optimization
method, Applied Mathematics and Computation \textbf{183}, pp. 260-271,
2006.

\bibitem{de_nn3}W. Zhang, W. Cai, FBSDE based neural network algorithms
for high-dimensional quasilinear parabolic PDEs, Journal of Computational
Physics \textbf{470}, 2022.

\bibitem{de_finite1}P. Frauenfelder, C. Schwab, R.A. Todor, Finite
elements for elliptic problems with stochastic coefficients, Computer
Methods in Applied Mechanics and Engineering \textbf{194}, pp. 205-228,
2005.

\bibitem{de_finite2}P. Houston, E. Süli, A note on the design of
hp-adaptive finite element methods for elliptic partial differential
equations, Computer Methods in Applied Mechanics and Engineering \textbf{194},
pp. 229-243, 2005.

\bibitem{ge_main}M. O'Neill, C. Ryan, Grammatical evolution, IEEE
Transactions on Evolutionary Computation \textbf{5}, pp. 349-358,
2001.

\bibitem{de_tsoulos}I.G. Tsoulos, I.E. Lagaris, Solving differential
equations with genetic programming. Genet Program Evolvable Mach \textbf{7},
pp. 33--54, 2006.

\bibitem{de_svm1}S. Mehrkanoon, T. Falck and J. A. K. Suykens, Approximate
Solutions to Ordinary Differential Equations Using Least Squares Support
Vector Machines, IEEE Transactions on Neural Networks and Learning
Systems \textbf{23}, pp. 1356-1367, 2012.

\bibitem{de_svm2}S. Mehrkanoon, J.A.K.Suykens, Learning solutions
to partial differential equations using LS-SVM, Neurocomputing \textbf{159},
pp. 105-116, 2015.

\bibitem{de_deep1}J. Sirignano, K. Spiliopoulos, DGM: A deep learning
algorithm for solving partial differential equations, Journal of Computational
Physics \textbf{375}, pp. 1339-1364, 2018.

\bibitem{de_deep2}M.Raissi, P.Perdikaris, G.E.Karniadakis, Physics-informed
neural networks: A deep learning framework for solving forward and
inverse problems involving nonlinear partial differential equations,
Journal of Computational Physics \textbf{378}, pp. 686-707, 2019.

\bibitem{de_nnc}I.G. Tsoulos, D. Gavrilis, E. Glavas, Solving differential
equations with constructed neural networks \textbf{72}, pp. 2385-2391,
2009.

\bibitem{RbfMain}J. Park and I. W. Sandberg, Universal Approximation
Using Radial-Basis-Function Networks, Neural Computation 3, pp. 246-257,
1991.

\bibitem{ga1}D. Goldberg, Genetic Algorithms in Search, Optimization
and Machine Learning, Addison-Wesley Publishing Company, Reading,
Massachussets, 1989.

\bibitem{ga2}Z. Michaelewicz, Genetic Algorithms + Data Structures
= Evolution Programs. Springer - Verlag, Berlin, 1996.

\bibitem{ga3}S.A. Grady, M.Y. Hussaini, M.M. Abdullah, Placement
of wind turbines using genetic algorithms, Renewable Energy \textbf{30},
pp. 259-270, 2005.

\bibitem{pso1}J. Kennedy, R.C. Eberhart, The particle swarm: social
adaptation in information processing systems, in: D. Corne, M. Dorigo
and F. Glover (eds.), New ideas in Optimization, McGraw-Hill, Cambridge,
UK, pp. 11-32, 1999.

\bibitem{pso2}F. Marini, B. Walczak, Particle swarm optimization
(PSO). A tutorial, Chemometrics and Intelligent Laboratory Systems
\textbf{149}, pp. 153-165, 2015.

\bibitem{nnphysics1}P. Baldi, K. Cranmer, T. Faucett et al, Parameterized
neural networks for high-energy physics, Eur. Phys. J. C \textbf{76},
2016.

\bibitem{nnphysics2}J. J. Valdas and G. Bonham-Carter, Time dependent
neural network models for detecting changes of state in complex processes:
Applications in earth sciences and astronomy, Neural Networks \textbf{19},
pp. 196-207, 2006

\bibitem{nnphysics3}G. Carleo,M. Troyer, Solving the quantum many-body
problem with artificial neural networks, Science \textbf{355}, pp.
602-606, 2017.

\bibitem{nnchem1}Lin Shen, Jingheng Wu, and Weitao Yang, Multiscale
Quantum Mechanics/Molecular Mechanics Simulations with Neural Networks,
Journal of Chemical Theory and Computation \textbf{12}, pp. 4934-4946,
2016.

\bibitem{nnchem2}Sergei Manzhos, Richard Dawes, Tucker Carrington,
Neural network-based approaches for building high dimensional and
quantum dynamics-friendly potential energy surfaces, Int. J. Quantum
Chem. \textbf{115}, pp. 1012-1020, 2015.

\bibitem{nnchem3}Jennifer N. Wei, David Duvenaud, and Alán Aspuru-Guzik,
Neural Networks for the Prediction of Organic Chemistry Reactions,
ACS Central Science \textbf{2}, pp. 725-732, 2016.

\bibitem{nnmed1}Igor I. Baskin, David Winkler and Igor V. Tetko,
A renaissance of neural networks in drug discovery, Expert Opinion
on Drug Discovery \textbf{11}, pp. 785-795, 2016.

\bibitem{nnmed2}Ronadl Bartzatt, Prediction of Novel Anti-Ebola Virus
Compounds Utilizing Artificial Neural Network (ANN), Chemistry Faculty
Publications \textbf{49}, pp. 16-34, 2018.

\bibitem{nnc}I.G. Tsoulos, D. Gavrilis, E. Glavas, Neural network
construction and training using grammatical evolution, Neurocomputing
\textbf{72}, pp. 269-277, 2008.

\bibitem{rbfphysics1}P. Teng, Machine-learning quantum mechanics:
Solving quantum mechanics problems using radial basis function networks,
Phys. Rev. E \textbf{98}, 033305, 2018.

\bibitem{rbfchemistry1}Chuanhao Wan and Peter de B. Harrington, Self-Configuring
Radial Basis Function Neural Networks for Chemical Pattern Recognition,
J. Chem. Inf. Comput. Sci. \textbf{39}, 1049--1056, 1999.

\bibitem{rbfmed1}Y.P. Wang, J.W. Dang, Q. Li and S. Li, Multimodal
medical image fusion using fuzzy radial basis function neural networks,
in 2007 International Conference on Wavelet Analysis and Pattern Recognition,
pp. 778-782, 2017.

\bibitem{gen_app1}R.L. Haupt, An introduction to genetic algorithms
for electromagnetics, Antennas and Propagation Magazine 37, pp. 7-15,
1995.

\bibitem{gen_app2}J.J. Grefenstette, R. Gopal, B. J. Rosmaita, D.
Van Gucht, Genetic Algorithms for the Traveling Salesman Problem,
In: Proceedings of the 1st International Conference on Genetic Algorithms,
pp. 160 - 168, Lawrence Erlbaum Associates, 1985.

\bibitem{gen_app3}D. A. Savic, G. A. Walters, Genetic Algorithms
for Least-Cost Design of Water Distribution Networks, Journal of Water
Resources Planning and Management 123, pp. 67-77, 1997.

\bibitem{pso_app1}H. Yoshida, K. Kawata, Y. Fukuyama, S. Takayama,
Y. Nakanishi, A particle swarm optimization for reactive power and
voltagecontrol considering voltage security assessment, IEEE Transactions
on Power Systems \textbf{15}, pp. 1232-1239, 2000.

\bibitem{pso_app2}J. Robinson, Y. Rahmat-Samii, Particle swarm optimization
in electromagnetics, IEEE Transactions on Antennas and Propagation
\textbf{52}, pp. 397- 407, 2004.

\bibitem{pso_app3}M. A. Abido, Optimal power flow using particle
swarm optimization, International Journal of Electrical Power \& Energy
Systems \textbf{24}, pp. 563-571, 2002.

\bibitem{pso_app4}Z.L. Gaing, Particle swarm optimization to solving
the economic dispatch considering the generator constraints, IEEE
Transactions on Power Systems \textbf{18}, pp. 1187-1195, 2003.

\bibitem{powell}M.J.D Powell, A Tolerant Algorithm for Linearly Constrained
Optimization Calculations, Mathematical Programming \textbf{45}, pp.
547-566, 1989. 

\bibitem{kaeloAli}P. Kaelo, M.M. Ali, Integrated crossover rules
in real coded genetic algorithms, European Journal of Operational
Research \textbf{176}, pp. 60-76, 2007.

\bibitem{openmpi}R. L. Graham, G. M. Shipman, B. W. Barrett, R. H.
Castain, G. Bosilca and A. Lumsdaine, \textquotedbl Open MPI: A High-Performance,
Heterogeneous MPI,\textquotedbl{} 2006 IEEE International Conference
on Cluster Computing, 2006, pp. 1-9.

\bibitem{openmp}L. Dagum, R. Menon, OpenMP: an industry standard
API for shared-memory programming, IEEE Computational Science and
Engineering \textbf{5}, pp. 46-55, 1998.
\end{thebibliography}

\end{document}
