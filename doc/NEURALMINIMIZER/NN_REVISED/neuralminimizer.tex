%% LyX 2.3.6 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{float}
\usepackage{url}
\usepackage{graphicx}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

\makeatother

\usepackage{babel}
\begin{document}
\title{NeuralMinimizer, a novel method for global optimization }
\author{Ioannis G. Tsoulos$^{(1)}$\thanks{Corresponding author. Email: itsoulos@uoi.gr},
Alexandros Tzallas$^{(1)}$, \\
Evangelos Karvounis$^{(1)}$, Dimitrios Tsalikakis$^{(2)}$}
\date{$^{(1)}$Department of Informatics and Telecommunications, University
of Ioannina, 47100 Arta, Greece \\
$^{(2)}$University of Western Macedonia, Department of Engineering
Informatics and Telecommunications, Greece}
\maketitle
\begin{abstract}
The problem of finding the global minimum of multidimensional functions
is often applied to a wide range of problems. An innovative method
of finding the global minimum of multidimensional functions is presented
here. This method first generates an approximation of the objective
function using only a few real samples from it. These samples construct
the approach using a machine learning model. Next, the required sampling
is performed by the approximation function. Furthermore, the approach
is improved on each sample by using found local minima as samples
for the training set of the machine learning model. In addition, the
proposed technique uses as a termination criterion a widely used criterion
from the relevant literature which in fact evaluates it after each
execution of the local minimization. The proposed technique was applied
to a number of well-known problems from the relevant literature, and
the comparative results with respect to modern global minimization
techniques are shown to be extremely promising.
\end{abstract}

\section{Introduction }

An innovative method for finding the global minimum of multidimensional
functions is presented here. The functions considered are continuous
and differentiable function and defined as $f:S\rightarrow R,S\subset R^{n}$.
The problem of locating the global optimum is usually formulated as:
\begin{equation}
x^{*}=\mbox{arg}\min_{x\in S}f(x)\label{eq:eq1}
\end{equation}
with $S$: 
\[
S=\left[a_{1},b_{1}\right]\otimes\left[a_{2},b_{2}\right]\otimes\ldots\left[a_{n},b_{n}\right]
\]
A variety of problems in the physical world can be represented as
global minimum problems, such as problems from physics \cite{go_physics1,go_physics2,go_physics3},
chemistry \cite{go_chemistry1,go_chemistry2,go_chemistry3}, economics
\cite{go_econ1,go_econ2}, medicine \cite{go_med1,go_med2} etc. During
the past years many methods, especially stochastic one, have been
proposed to tackle the problem of equation \ref{eq:eq1}, such as
Controlled Random Search methods \cite{crs1,crs2,crs3} , Simulated
Annealing methods \cite{simann_major,simann1,simann2}, Differential
Evolution methods \cite{diffe1,diffe2}, Particle Swarm Optimization
(PSO) methods \cite{pso_major,pso1,pso2}, Ant Colony Optimization
\cite{aco1,aco2}, Genetic algorithms \cite{ga1,ga2,ga3} etc. A systematic
review of global optimization methods can also be found in the work
of Floudas et al \cite{FloudasReview}. In addition, during the last
few years, a variety of work has been proposed on combinations and
modifications to some global optimization methods to more efficiently
find the global minimum, such as methods that combine PSO with other
methods \cite{pso_hybrid1,pso_hybrid2,pso_hybrid3}, methods aimed
to discover all the local minima of functions \cite{tmlsl,Salhi,minfinder},
new stopping rules to efficiently terminate the global optimization
techniques \cite{msstop1,msstop2,msstop3} etc. Also, due to the massive
use of parallel processing techniques, several methods have been proposed
that take full advantage of parallel processing, such as parallel
techniques \cite{parallel-pso,parallel-multistart,parallel-doublepop},
methods that utilize the GPU architectures \cite{msgpu1,msgpu2} etc. 

Also, during the past years, many metaheuristic algorithms have appeared
to tackle to global optimization problems such as Quantum-based avian
navigation optimizer algorithm \cite{qana}, a Tunicate Swarm Algorithm
(TSA) inspired by simulating the lives of Tunicates at sea and how
food is obtained \cite{tsa}, Starling murmuration optimizer \cite{starling1,starling2},
the Diversity-maintained multi-trial vector differential evolution
algorithm (DMDE) algorithm used in large scale global optimization
\cite{dmde}, an improved moth-flame optimization algorithm with adaptation
mechanism to solve numerical and mechanical engineering problems \cite{flame},
the dwarf mongoose optimization algorithm \cite{dwarf} etc.

In this paper, a new multi-start method is proposed that uses a machine
learning model, which is trained in parallel with the evolution of
the optimization process. Although multistart methods are considered
the basis for more modern optimization techniques, they have been
successfully used in several problems such as the Traveling Salesman
Problem (TSP)\textbf{ }\cite{multistart-tsp1,multistart_tsp2,multistart_tsp3},
the maximum clique problem \cite{ms_clique1,ms_clique2}, the vehicle
routing problem \cite{multistart-vehicle1,multistart-vehicle2}, scheduling
problems \cite{ms_schedule1,ms_schedule2} etc. In the new technique,
a Radial Basis Function (RBF) network \cite{rbf_main1} is used to
construct an approximation of the objective function. This construction
is carried out in parallel with the execution of the optimization.
A limited number of samples from the objective function and the local
minima discovered during the optimization are used to construct the
approximation function. During the execution of the method, the samples
needed to start local minimizers are taken from the approximation
function that is constructed by the neural network. The RBF network
was used as an approximation tool as it has been successfully used
in a wide range of problems in the field of artificial intelligence
\cite{rbf_face,rbf_function1,rbf_function2,rbf_image} and its training
procedure is very fast, if compared to artificial neural networks
for example. In addition, for more efficient termination of the method,
a termination method proposed by Tsoulos is used \cite{Tsoulos1},
but this termination method is applied after each execution of the
local minimization procedure. The mentioned method was applied to
some test functions provided by the relevant literature and the results
are extremely promising as compared with other global optimization
techniques.

The proposed method does not sample the actual function but an approximation
of it, which is generated incrementally. The creation of the approximation
is done by using an RBF neural network, known for its reliability
and its ability to efficiently approximate functions. The initial
approximation is created from a limited number of points and then
it will be improved through the local minimizers that will be found
during the execution of the method. With the above procedure, the
required number of function calls is drastically reduced, since the
actual function is not used to produce samples, but an approximation
of it. Only samples with low function values are taken from the approximation
function, which means that finding the global minimum is likely to
be performed faster than other techniques and more efficiently. Furthermore,
the generation of the approximation function does not use any prior
knowledge about the objective problem.

The rest of this article is organized as follows: in section \ref{sec:Method-Description}
the description of the proposed method is provided, in section \ref{sec:Experiments}
the used experimental functions as well as the experimental results
and comparisons are listed and finally in section \ref{sec:Conclusions}
some conclusions and final thoughts are given.

\section{Method Description \label{sec:Method-Description}}

The proposed technique generates an estimation of the objective function
during the optimization using an RBF network. This estimation is initially
generated from some samples from the objective function and gradually
local minima that will have been discovered during the optimization
are added to it. In this way, the estimation of the objective function
will be continuously improved to approximate the true function as
much as possible. At every iteration, several samples are then taken
from the estimated function and sorted in ascending order. Those with
the lowest value will be starting points of the local minimization
method. The local optimization method used here is a BFGS variant
of Powell \cite{powell}. This process has the effect of drastically
reducing the total number of function calls that are made and, at
the same time, the points used as initiators of the local minimization
technique approach the global minimum of the objective function. Also,
the proposed method checks the termination rule after the application
of every local search method. That way, if the absolute minimum has
already been discovered with some certainty, no more function calls
will be wasted finding it.

In the following subsections, the training procedure of RBF networks
as well as the proposed method are fully described.

\subsection{RBF preliminaries}

An RBF network can be defined as:\textbf{
\begin{equation}
N\left(\overrightarrow{x}\right)=\sum_{i=1}^{k}w_{i}\phi\left(\left\Vert \overrightarrow{x}-\overrightarrow{c_{i}}\right\Vert \right)\label{eq:firstrbf}
\end{equation}
}where 
\begin{enumerate}
\item The vector $\overrightarrow{x}$ is called the input pattern to the
equation. 
\item The vectors $\overrightarrow{c_{i}},\ i=1,..,k$ are called the center
vectors.
\item The vector $\overrightarrow{w}$ stands for the the output weight
of the RBF network.
\end{enumerate}
In most cases the function $\phi(x)$ is a Gaussian function:\textbf{
\begin{equation}
\phi(x)=\exp\left(-\frac{\left(x-c\right)^{2}}{\sigma^{2}}\right)
\end{equation}
} The training error for the RBF network on a set $T=\left\{ \left(x_{1},y_{1}\right),\left(x_{2},y_{2}\right),\ldots,\left(x_{M},y_{M}\right)\right\} $
points is estimated as
\begin{equation}
E\left(N\left(\overrightarrow{x}\right)\right)=\sum_{i=1}^{M}\left(N\left(x_{i}\right)-y_{i}\right)^{2}\label{eq:rbf_error}
\end{equation}
In most approaches, the equation \ref{eq:rbf_error} is minimized
with respect to the parameters of the RBF network using a two phase
procedure:
\begin{enumerate}
\item In the first phase, the K-Means algorithm \cite{kmeans} is used to
approximate the $k$ centers and the corresponding variances.
\item In the second phase, the weight vector $\overrightarrow{w}=\left(w_{1},w_{2},\ldots,w_{k}\right)$
is calculated by solving a linear system of equations as follows:
\begin{enumerate}
\item \textbf{Set} \textbf{$W=w_{kj}$}
\item \textbf{Set} \textbf{$\Phi=\phi_{j}\left(x_{i}\right)$}
\item \textbf{Set $T=\left\{ t_{i}=f\left(x_{i}\right),i=1,..,M\right\} $. }
\item The system to be solved is identified as:\textbf{ 
\begin{equation}
\Phi^{T}\left(T-\Phi W^{T}\right)=0
\end{equation}
}With solution:\textbf{
\begin{equation}
W^{T}=\left(\Phi^{T}\Phi\right)^{-1}\Phi^{T}T=\Phi^{\dagger}T\label{eq:eqoutput}
\end{equation}
}
\end{enumerate}
\end{enumerate}

\subsection{The main algorithm}

The main steps of the proposed algorithm have as follows:
\begin{enumerate}
\item \textbf{Initialization} step.
\begin{enumerate}
\item \textbf{Set} $k$ the number of weights in RBF network.
\item \textbf{Set} $N_{S}$ the initial samples that will be taken from
the function $f(x)$.
\item \textbf{Set} $N_{T}$ the number of samples, that will used in every
iteration as starting points for the local optimization procedure.
\item Set $N_{R}$ the number of samples that will be drawn from the RBF
network at every iteration with $N_{R}>N_{T}$
\item \textbf{Set} $N_{G}$ the maximum number of allowed iterations.
\item \textbf{Set} Iter=0, the iteration number.
\item \textbf{Set} $\left(x^{*},y^{*}\right)$ as the global minimum. Initially
$y^{*}=\infty$
\end{enumerate}
\item \textbf{Creation }Step.
\begin{enumerate}
\item \textbf{Set} $T=\emptyset$, the training set for the RBF network.
\item \textbf{For} $i=1,..,N_{S}$ do
\begin{enumerate}
\item \textbf{Take} a new sample $x_{i}\in S$
\item \textbf{Calculate} $y_{i}=f\left(x_{i}\right)$
\item $T=T\cup\left(x_{i},y_{i}\right)$
\end{enumerate}
\item \textbf{EndFor}
\item \textbf{Train} the RBF network on the training set $T$.
\end{enumerate}
\item \textbf{Sampling }Step
\begin{enumerate}
\item \textbf{Set} $T_{R}=\emptyset$
\item \textbf{For} $i=1,..,N_{R}$ \textbf{do}
\begin{enumerate}
\item \textbf{Take} a random sample $\left(x_{i},y_{i}\right)$ from the
RBF network.
\item \textbf{Set} $T_{R}=T_{R}\cup\left(x_{i,}y_{i}\right)$
\end{enumerate}
\item \textbf{EndFor}
\item \textbf{Sort} $T_{R}$ according to the y values in ascending order.
\end{enumerate}
\item \textbf{Optimization} Step.
\begin{enumerate}
\item \textbf{For} $i=1,..,N_{T}$ do
\begin{enumerate}
\item \textbf{Take} the next sample $\left(x_{i},y_{i}\right)$ from $T_{R}$.
\item $y_{i}=\mbox{LS}\left(x_{i}\right)$. Where LS(x) is a predefined
local search method.
\item $T=T\cup\left(x_{i},y_{i}\right)$, this step updates the training
set of the RBF network.
\item \textbf{Train} the RBF network on set $T$.
\item \textbf{If} $y_{i}\le y^{*}$ then $x^{*}=x_{i},y^{*}=y_{i}$
\item \textbf{Check} the termination rule as suggested in \cite{Tsoulos1}.
If it holds report $\left(x^{*},y^{*}\right)$ as the located global
minimum and terminate.
\end{enumerate}
\item \textbf{EndFor}
\end{enumerate}
\item \textbf{Set} iter=iter+1
\item \textbf{Goto} to Sampling step.
\end{enumerate}
The steps of the algorithm are illustrated graphically in Figure \ref{fig:steps}.

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.4]{diagram}
\par\end{centering}
\caption{The steps of the proposed algorithm.\label{fig:steps}}

\end{figure}


\section{Experiments \label{sec:Experiments}}

To estimate the efficiency of the new technique, a number of functions
from the relevant literature were used \cite{Ali1,Floudas1}. These
functions are provided in the Appendix. The proposed technique was
tested on these test functions and the results produced were compared
with those given by a simple genetic algorithm or a PSO method or
differential evolution (DE) method. The used genetic algorithm is
based on the $\mbox{GA}\left(c_{r1},l\right)$ algorithm from the
work of Kaelo and Ali \cite{kaelo}. In order to have fairness in
the comparison of the results, for all global optimization techniques
the same local minimization method has been used as that of the proposed
method. In addition, the number of chromosomes in the genetic algorithm
and the number of particles in the PSO method are identical to the
parameter $N_{T}$ of the proposed procedure. Also, for the DE method,
the number of agents was set to $N_{T}$. The values for the parameters
used in the conducted experiments are shown in Table \ref{tab:Experimental-settings.}.
For every function and for every global optimizer, 30 independent
runs were executed using a different seed for the random generator
each time. The proposed method is implemented as the method with the
name \emph{NeuralMinimizer} in the \emph{OPTIMUS} global optimization
environment, which is freely available from \url{https://github.com/itsoulos/OPTIMUS}.
All the experiments were conducted on an AMD Ryzen 5950X with 128GB
of RAM and the Debian Linux operating system. 

The experimental results from the application of the proposed method
and the other methods are shown in Table \ref{tab:Comparison}. The
number in the cells represent average function calls. The number in
parentheses indicates the fraction of runs where the global optimum
was successfully discovered. Absence of this fraction indicated that
the global minimum is discovered for every execution (100\% success).
At the end of the table, an additional row named AVERAGE has been
added to show the total number of function calls and the average success
rate in locating the global minimum. In the experimental results,
the superiority of the proposed technique over the other two methods
in terms of the number of function calls is clear. The proposed technique
requires an average of 90\% fewer function calls than the other methods.
In addition, the proposed technique appears to be more efficient than
the other two as it finds more times on average the global minimum
of most test functions in the experiments. Also, the statistical comparison
between the global optimization methods is shown in Figure \ref{fig:Statistical}.

In addition, the table \ref{tab:kexperiments} presents the experimental
results for the proposed method and for various values of the parameter
$N_{S}$. As can be seen, the increase in this parameter does not
cause a large increase in the total number of function calls, while
at the same time it improves to some extent the ability of the proposed
technique to find the global minimum.

The efficiency of the method is also shown in Table \ref{tab:potential},
where the proposed method is compared against genetic algorithm and
particle swarm optimization for a range of number of atoms of the
Potential problem. As can be seen in the table, the proposed method
requires a significantly smaller number of function calls compared
to the other techniques and its reliability in finding the global
minimum remains high even when the number of people in the potential
increases significantly.

\begin{table}[H]
\caption{Experimental settings.\label{tab:Experimental-settings.}}

\centering{}%
\begin{tabular}{|c|c|c|}
\hline 
PARAMETER & MEANING & VALUE\tabularnewline
\hline 
\hline 
$k$ & Number of weights & 10\tabularnewline
\hline 
$N_{S}$ & Start samples & 50\tabularnewline
\hline 
$N_{T}$ & Number of samples used as starting points & 100\tabularnewline
\hline 
$N_{R}$ & Number of samples that will be drawn from the RBF network & $10\times N_{T}$\tabularnewline
\hline 
$N_{C}$ & Chromosomes or Particles or agents & 100\tabularnewline
\hline 
$N_{G}$ & Maximum number of iterations & 200\tabularnewline
\hline 
\end{tabular}
\end{table}

\begin{table}[H]
\caption{Comparison between the proposed method and the Genetic and PSO methods.\label{tab:Comparison}}

\centering{}%
\begin{tabular}{|c|c|c|c|c|}
\hline 
\textbf{FUNCTION} & \textbf{GENETIC} & \textbf{PSO} & \textbf{DE} & \textbf{PROPOSED}\tabularnewline
\hline 
\hline 
BF1 & 7150 & 9030(0.87) & 5579 & 1051\tabularnewline
\hline 
BF2 & 7504 & 6505(0.67) & 5598 & 921\tabularnewline
\hline 
BRANIN & 6135 & 6865(0.93) & 5888 & 460\tabularnewline
\hline 
CAMEL & 6564 & 5162 & 6403 & 778\tabularnewline
\hline 
CIGAR10 & 11813 & 18803 & 13313 & 1896\tabularnewline
\hline 
CM4 & 10537 & 11124 & 9018 & 1877(0.87)\tabularnewline
\hline 
DISCUS10 & 20208 & 6039 & 7797 & 478\tabularnewline
\hline 
EASOM & 5281 & 2037 & 7917 & 258\tabularnewline
\hline 
ELP10 & 20337 & 16731 & 2863 & 2263\tabularnewline
\hline 
EXP4 & 10537 & 9155 & 5944 & 750\tabularnewline
\hline 
EXP16 & 20131 & 14061 & 3653 & 885\tabularnewline
\hline 
EXP64 & 20140 & 8958 & 3692 & 948\tabularnewline
\hline 
GRIEWANK10 & 20151(0.10) & 17497(0.03) & 16469(0.03) & 2697\tabularnewline
\hline 
POTENTIAL3 & 18902 & 9936 & 5452 & 1192\tabularnewline
\hline 
POTENTIAL5 & 18477 & 12385 & 3972 & 2399\tabularnewline
\hline 
HANSEN & 10708 & 9104 & 14016 & 2370(0.93)\tabularnewline
\hline 
HARTMAN3 & 8481 & 12971 & 4677 & 642\tabularnewline
\hline 
HARTMAN6 & 17723(0.60) & 15174(0.57) & 14372(0.90) & 883\tabularnewline
\hline 
RASTRIGIN & 6744 & 7639(0.97) & 6148 & 1408(0.80)\tabularnewline
\hline 
ROSENBROCK4 & 20815(0.63) & 11526 & 16763 & 1619\tabularnewline
\hline 
ROSENBROCK8 & 20597(0.67) & 16967 & 16631 & 2444\tabularnewline
\hline 
SHEKEL5 & 14456(0.73) & 15082(0.47) & 13178 & 2333(0.87)\tabularnewline
\hline 
SHEKEL7 & 16786(0.83) & 14625(0.40) & 12050 & 1844(0.93)\tabularnewline
\hline 
SHEKEL10 & 15586(0.80) & 12628(0.53) & 13107 & 2451\tabularnewline
\hline 
SINU4 & 11908 & 10659 & 9048 & 802\tabularnewline
\hline 
SINU8 & 20115 & 13912 & 16210 & 1500(0.97)\tabularnewline
\hline 
TEST2N4 & 13943 & 12948 & 10864 & 878(0.93)\tabularnewline
\hline 
TEST2N5 & 15814 & 13936(0.90) & 15259 & 971(0.77)\tabularnewline
\hline 
TEST2N6 & 18987 & 15449(0.70) & 12839 & 997(0.70)\tabularnewline
\hline 
TEST2N7 & 20035 & 16020(0.50) & 8185(0.97) & 1084(0.30)\tabularnewline
\hline 
TEST30N3 & 13029 & 7239 & 4839 & 1061\tabularnewline
\hline 
TEST30N4 & 12889 & 8051 & 5070 & 854\tabularnewline
\hline 
\textbf{AVERAGE} & \textbf{472596(0.89)} & \textbf{368218(0.86)} & \textbf{296814(0.96)} & \textbf{42994(0.94)}\tabularnewline
\hline 
\end{tabular}
\end{table}

\begin{figure}[H]
\begin{centering}
\includegraphics{boxplot}
\par\end{centering}
\caption{Statistical comparison between the global optimization methods.\label{fig:Statistical}}

\end{figure}

\begin{table}[H]
\caption{Experimental results for the proposed method and for different values
of the critical parameter $N_{S}$ (50, 100, 200). Numbers in cells
represent averages of 30 runs.\label{tab:kexperiments}}

\centering{}%
\begin{tabular}{|c|c|c|c|}
\hline 
\textbf{FUNCTION} & \textbf{$N_{S}=50$} & \textbf{$N_{S}=100$} & \textbf{$N_{S}=200$}\tabularnewline
\hline 
\hline 
BF1 & 1051 & 1116 & 1224\tabularnewline
\hline 
BF2 & 921 & 949 & 1058\tabularnewline
\hline 
BRANIN & 460 & 506 & 599\tabularnewline
\hline 
CAMEL & 778 & 676 & 739\tabularnewline
\hline 
CIGAR10 & 1896 & 1934 & 2042\tabularnewline
\hline 
CM4 & 1877(0.87) & 1859(0.93) & 1877(0.90)\tabularnewline
\hline 
DISCUS10 & 478 & 531 & 634\tabularnewline
\hline 
EASOM & 258 & 307 & 450\tabularnewline
\hline 
ELP10 & 2263 & 2339 & 3130\tabularnewline
\hline 
EXP4 & 750 & 778 & 884\tabularnewline
\hline 
EXP16 & 885 & 932 & 1030\tabularnewline
\hline 
EXP64 & 948 & 998 & 1091\tabularnewline
\hline 
GRIEWANK10 & 2697 & 2647 & 2801\tabularnewline
\hline 
POTENTIAL3 & 1192 & 1228 & 1305\tabularnewline
\hline 
POTENTIAL5 & 2399 & 2417 & 2544\tabularnewline
\hline 
HANSEN & 2370(0.93) & 2602(0.93) & 2578(0.97)\tabularnewline
\hline 
HARTMAN3 & 642 & 696 & 798\tabularnewline
\hline 
HARTMAN6 & 883 & 940 & 1038\tabularnewline
\hline 
RASTRIGIN & 1408(0.80) & 989(0.83) & 1041\tabularnewline
\hline 
ROSENBROCK4 & 1619 & 1674 & 1751\tabularnewline
\hline 
ROSENBROCK8 & 2444 & 2499 & 2583\tabularnewline
\hline 
SHEKEL5 & 2333(0.87) & 1267 & 1878(0.97)\tabularnewline
\hline 
SHEKEL7 & 1844(0.93) & 1517(0.93) & 1685(0.97)\tabularnewline
\hline 
SHEKEL10 & 2451 & 2695 & 1498\tabularnewline
\hline 
SINU4 & 802 & 821 & 901\tabularnewline
\hline 
SINU8 & 1500(0.97) & 1216 & 1247\tabularnewline
\hline 
TEST2N4 & 878(0.93) & 934 & 850(0.97)\tabularnewline
\hline 
TEST2N5 & 971(0.77) & 941(0.80) & 993\tabularnewline
\hline 
TEST2N6 & 997(0.70) & 1087(0.77) & 1098\tabularnewline
\hline 
TEST2N7 & 1084(0.30) & 1160(0.53) & 1313(0.57)\tabularnewline
\hline 
TEST30N3 & 1061 & 998 & 1320\tabularnewline
\hline 
TEST30N4 & 854 & 830 & 1108\tabularnewline
\hline 
\textbf{AVERAGE} & \textbf{42994(0.94)} & \textbf{42083(0.96)} & \textbf{45088(0.97)}\tabularnewline
\hline 
\end{tabular}
\end{table}

\begin{table}[H]

\caption{Optimizing the Potential problem for different number of atoms.\label{tab:potential} }

\centering{}%
\begin{tabular}{|c|c|c|c|}
\hline 
ATOMS & GENETIC & PSO & PROPOSED\tabularnewline
\hline 
\hline 
3 & 18902 & 9936 & 1192\tabularnewline
\hline 
4 & 17806 & 12560 & 1964\tabularnewline
\hline 
5 & 18477 & 12385 & 2399\tabularnewline
\hline 
6 & 19069(0.20) & 9683 & 3198\tabularnewline
\hline 
7 & 16390(0.33) & 10533(0.17) & 3311(0.97)\tabularnewline
\hline 
8 & 15924(0.50) & 8053(0.50) & 3526\tabularnewline
\hline 
9 & 15041(0.27) & 9276(0.17) & 4338\tabularnewline
\hline 
10 & 14817(0.03) & 7548(0.17) & 5517(0.87)\tabularnewline
\hline 
11 & 13885(0.03) & 6864(0.13) & 6588(0.80)\tabularnewline
\hline 
12 & 14435(0.17) & 12182(0.07) & 7508(0.83)\tabularnewline
\hline 
13 & 14457(0.07) & 10748(0.03) & 6717(0.77)\tabularnewline
\hline 
14 & 13906(0.07) & 14235(0.13) & 6201(0.93)\tabularnewline
\hline 
15 & 12832(0.10) & 12980(0.10) & 7802(0.90)\tabularnewline
\hline 
\textbf{AVERAGE} & \textbf{205941(0.37)} & \textbf{137134(0.42)} & \textbf{60258(0.93)}\tabularnewline
\hline 
\end{tabular}
\end{table}


\section{Conclusions \label{sec:Conclusions}}

An innovative technique for finding the global minimum of multidimensional
functions was presented in this work. This new technique is based
on the multistart procedure, but also generates an estimation of the
objective function through a machine learning model. The machine learning
model constructs an estimation of the objective function using a small
number of samples from the true function but also with the contribution
of local minima discovered during the execution of the method. In
this way, the estimation of the objective function is continuously
improved and the sampling to perform local minimization is done from
the estimated function rather than the actual one. This procedure
combined with checking the termination criterion after each execution
of the local minimization method led the proposed method to have excellent
results both in terms of the speed of finding the global minimum and
its efficiency. In addition, the method shows significant stability
in its performance even in large changes of its parameters as presented
in the experimental results section.

In the future, the use of the RBF network to construct an approximation
of the objective function can be applied to more modern optimization
techniques such as genetic algorithms. It would also be interesting
to create a parallel implementation of the proposed method, in order
to significantly speed up its execution and to be able to be used
efficiently in optimization problems of higher dimensions.

\section*{Appendix A.}
\begin{itemize}
\item \textbf{\emph{Bent Cigar function}}\emph{ }The function is 
\[
f(x)=x_{1}^{2}+10^{6}\sum_{i=2}^{n}x_{i}^{2}
\]
 with the global minimum $f\left(x^{*}\right)=0$. For the conducted
experiments the value $n=10$ was used.
\item \textbf{Bf1} function. The function Bohachevsky 1 is given by the
equation
\end{itemize}
\[
f(x)=x_{1}^{2}+2x_{2}^{2}-\frac{3}{10}\cos\left(3\pi x_{1}\right)-\frac{4}{10}\cos\left(4\pi x_{2}\right)+\frac{7}{10}
\]
with $x\in[-100,100]^{2}$. 
\begin{itemize}
\item \textbf{Bf2} function. The function Bohachevsky 2 is given by the
equation 
\[
f(x)=x_{1}^{2}+2x_{2}^{2}-\frac{3}{10}\cos\left(3\pi x_{1}\right)\cos\left(4\pi x_{2}\right)+\frac{3}{10}
\]
with $x\in[-50,50]^{2}$. 
\item \textbf{Branin} function. The function is defined by $f(x)=\left(x_{2}-\frac{5.1}{4\pi^{2}}x_{1}^{2}+\frac{5}{\pi}x_{1}-6\right)^{2}+10\left(1-\frac{1}{8\pi}\right)\cos(x_{1})+10$
with $-5\le x_{1}\le10,\ 0\le x_{2}\le15$. The value of global minimum
is 0.397887.with $x\in[-10,10]^{2}$. 
\item \textbf{CM} function. The Cosine Mixture function is given by the
equation 
\[
f(x)=\sum_{i=1}^{n}x_{i}^{2}-\frac{1}{10}\sum_{i=1}^{n}\cos\left(5\pi x_{i}\right)
\]
with $x\in[-1,1]^{n}$. For the conducted experiments the value $n=4$
was used.
\item \textbf{Camel} function. The function is given by 
\[
f(x)=4x_{1}^{2}-2.1x_{1}^{4}+\frac{1}{3}x_{1}^{6}+x_{1}x_{2}-4x_{2}^{2}+4x_{2}^{4},\quad x\in[-5,5]^{2}
\]
The global minimum has the value of $f\left(x^{*}\right)=-1.0316$
\item \textbf{Discus function}\emph{ }The function is defined as 
\[
f(x)=10^{6}x_{1}^{2}+\sum_{i=2}^{n}x_{i}^{2}
\]
 with global minimum $f\left(x^{*}\right)=0.$ For the conducted experiments
the value $n=10$ was used.
\item \textbf{Easom} function The function is given by the equation 
\[
f(x)=-\cos\left(x_{1}\right)\cos\left(x_{2}\right)\exp\left(\left(x_{2}-\pi\right)^{2}-\left(x_{1}-\pi\right)^{2}\right)
\]
with $x\in[-100,100]^{2}$ and global minimum -1.0
\item \textbf{Exponential} function. The function is given by 
\[
f(x)=-\exp\left(-0.5\sum_{i=1}^{n}x_{i}^{2}\right),\quad-1\le x_{i}\le1
\]
The values $n=4,16,64$ was used here and the corresponding functions
names are EXP4, EXP16, EXP64.
\item \textbf{Griewank10} function, defined as:
\[
f(x)=\sum_{i=1}^{n}\frac{x_{i}^{2}}{4000}-\prod_{i=1}^{n}\cos\left(\frac{x_{i}}{\sqrt{i}}\right)+1
\]
with $n=10$.
\item \textbf{Hansen} function. $f(x)=\sum_{i=1}^{5}i\cos\left[(i-1)x_{1}+i\right]\sum_{j=1}^{5}j\cos\left[(j+1)x_{2}+j\right]$,
$x\in[-10,10]^{2}$ . The global minimum of the function is -176.541793.
\item \textbf{Hartman 3} function. The function is given by
\[
f(x)=-\sum_{i=1}^{4}c_{i}\exp\left(-\sum_{j=1}^{3}a_{ij}\left(x_{j}-p_{ij}\right)^{2}\right)
\]
with $x\in[0,1]^{3}$ and $a=\left(\begin{array}{ccc}
3 & 10 & 30\\
0.1 & 10 & 35\\
3 & 10 & 30\\
0.1 & 10 & 35
\end{array}\right),\ c=\left(\begin{array}{c}
1\\
1.2\\
3\\
3.2
\end{array}\right)$ and
\[
p=\left(\begin{array}{ccc}
0.3689 & 0.117 & 0.2673\\
0.4699 & 0.4387 & 0.747\\
0.1091 & 0.8732 & 0.5547\\
0.03815 & 0.5743 & 0.8828
\end{array}\right)
\]
The value of global minimum is -3.862782.
\item \textbf{Hartman 6} function.
\[
f(x)=-\sum_{i=1}^{4}c_{i}\exp\left(-\sum_{j=1}^{6}a_{ij}\left(x_{j}-p_{ij}\right)^{2}\right)
\]
with $x\in[0,1]^{6}$ and $a=\left(\begin{array}{cccccc}
10 & 3 & 17 & 3.5 & 1.7 & 8\\
0.05 & 10 & 17 & 0.1 & 8 & 14\\
3 & 3.5 & 1.7 & 10 & 17 & 8\\
17 & 8 & 0.05 & 10 & 0.1 & 14
\end{array}\right),\ c=\left(\begin{array}{c}
1\\
1.2\\
3\\
3.2
\end{array}\right)$ and
\[
p=\left(\begin{array}{cccccc}
0.1312 & 0.1696 & 0.5569 & 0.0124 & 0.8283 & 0.5886\\
0.2329 & 0.4135 & 0.8307 & 0.3736 & 0.1004 & 0.9991\\
0.2348 & 0.1451 & 0.3522 & 0.2883 & 0.3047 & 0.6650\\
0.4047 & 0.8828 & 0.8732 & 0.5743 & 0.1091 & 0.0381
\end{array}\right)
\]
the value of global minimum is -3.322368.
\item \textbf{High Conditioned Elliptic} function, defined as 
\[
f(x)=\sum_{i=1}^{n}\left(10^{6}\right)^{\frac{i-1}{n-1}}x_{i}^{2}
\]
with $n=10$ for the conducted experiments.
\item \textbf{Potential} function, used to represent the lowest energy for
the molecular conformation of N atoms via the Lennard-Jones potential\cite{Jones}.
The function is defined as:
\begin{equation}
V_{LJ}(r)=4\epsilon\left[\left(\frac{\sigma}{r}\right)^{12}-\left(\frac{\sigma}{r}\right)^{6}\right]\label{eq:potential}
\end{equation}
In the current experiments two different cases were studied: $N=3,\ 5$
\item \textbf{Rastrigin} function. The function is given by 
\[
f(x)=x_{1}^{2}+x_{2}^{2}-\cos(18x_{1})-\cos(18x_{2}),\quad x\in[-1,1]^{2}
\]
\item \textbf{Shekel 7} function.
\end{itemize}
\[
f(x)=-\sum_{i=1}^{7}\frac{1}{(x-a_{i})(x-a_{i})^{T}+c_{i}}
\]

with $x\in[0,10]^{4}$ and $a=\left(\begin{array}{cccc}
4 & 4 & 4 & 4\\
1 & 1 & 1 & 1\\
8 & 8 & 8 & 8\\
6 & 6 & 6 & 6\\
3 & 7 & 3 & 7\\
2 & 9 & 2 & 9\\
5 & 3 & 5 & 3
\end{array}\right),\ c=\left(\begin{array}{c}
0.1\\
0.2\\
0.2\\
0.4\\
0.4\\
0.6\\
0.3
\end{array}\right)$. 
\begin{itemize}
\item \textbf{Shekel 5 }function.
\end{itemize}
\[
f(x)=-\sum_{i=1}^{5}\frac{1}{(x-a_{i})(x-a_{i})^{T}+c_{i}}
\]
 

with $x\in[0,10]^{4}$ and $a=\left(\begin{array}{cccc}
4 & 4 & 4 & 4\\
1 & 1 & 1 & 1\\
8 & 8 & 8 & 8\\
6 & 6 & 6 & 6\\
3 & 7 & 3 & 7
\end{array}\right),\ c=\left(\begin{array}{c}
0.1\\
0.2\\
0.2\\
0.4\\
0.4
\end{array}\right)$. 
\begin{itemize}
\item \textbf{Shekel 10} function.
\end{itemize}
\[
f(x)=-\sum_{i=1}^{10}\frac{1}{(x-a_{i})(x-a_{i})^{T}+c_{i}}
\]
 

with $x\in[0,10]^{4}$ and $a=\left(\begin{array}{cccc}
4 & 4 & 4 & 4\\
1 & 1 & 1 & 1\\
8 & 8 & 8 & 8\\
6 & 6 & 6 & 6\\
3 & 7 & 3 & 7\\
2 & 9 & 2 & 9\\
5 & 5 & 3 & 3\\
8 & 1 & 8 & 1\\
6 & 2 & 6 & 2\\
7 & 3.6 & 7 & 3.6
\end{array}\right),\ c=\left(\begin{array}{c}
0.1\\
0.2\\
0.2\\
0.4\\
0.4\\
0.6\\
0.3\\
0.7\\
0.5\\
0.6
\end{array}\right)$. 
\begin{itemize}
\item \textbf{Sinusoidal} function. The function is given by 
\[
f(x)=-\left(2.5\prod_{i=1}^{n}\sin\left(x_{i}-z\right)+\prod_{i=1}^{n}\sin\left(5\left(x_{i}-z\right)\right)\right),\quad0\le x_{i}\le\pi.
\]
The global minimum is located at $x^{*}=(2.09435,2.09435,...,2.09435)$
with $f\left(x^{*}\right)=-3.5$. For the conducted experiments the
cases of $n=4,8$ and $z=\frac{\pi}{6}$ were studied.
\item \textbf{Test2N} function. This function is given by the equation 
\[
f(x)=\frac{1}{2}\sum_{i=1}^{n}x_{i}^{4}-16x_{i}^{2}+5x_{i},\quad x_{i}\in[-5,5].
\]
The function has $2^{n}$ in the specified range and in our experiments
we used $n=4,5,6,7$. 
\item \textbf{Test30N} function. This function is given by 
\[
f(x)=\frac{1}{10}\sin^{2}\left(3\pi x_{1}\right)\sum_{i=2}^{n-1}\left(\left(x_{i}-1\right)^{2}\left(1+\sin^{2}\left(3\pi x_{i+1}\right)\right)\right)+\left(x_{n}-1\right)^{2}\left(1+\sin^{2}\left(2\pi x_{n}\right)\right)
\]
with $x\in[-10,10]$. The function has $30^{n}$ local minima in the
specified range and we used $n=3,4$ in the conducted experiments.
\end{itemize}
\begin{thebibliography}{10}
\bibitem{go_physics1}M. Honda, Application of genetic algorithms
to modelings of fusion plasma physics, Computer Physics Communications
\textbf{231}, pp. 94-106, 2018.

\bibitem{go_physics2}X.L. Luo, J. Feng, H.H. Zhang, A genetic algorithm
for astroparticle physics studies, Computer Physics Communications
\textbf{250}, 106818, 2020.

\bibitem{go_physics3}T.M. Aljohani, A.F. Ebrahim, O. Mohammed, Single
and Multiobjective Optimal Reactive Power Dispatch Based on Hybrid
Artificial Physics--Particle Swarm Optimization, Energies \textbf{12},
2333, 2019.

\bibitem{go_chemistry1}P.M. Pardalos, D. Shalloway, G. Xue, Optimization
methods for computing global minima of nonconvex potential energy
functions, Journal of Global Optimization \textbf{4}, pp. 117-133,
1994.

\bibitem{go_chemistry2}A. Liwo, J. Lee, D.R. Ripoll, J. Pillardy,
H. A. Scheraga, Protein structure prediction by global optimization
of a potential energy function, Biophysics \textbf{96}, pp. 5482-5485,
1999.

\bibitem{go_chemistry3}J. An, G.He, F. Qin, R. Li, Z. Huang, A new
framework of global sensitivity analysis for the chemical kinetic
model using PSO-BPNN, Computers \& Chemical Engineering \textbf{112},
pp. 154-164, 2018.

\bibitem{go_econ1}Zwe-Lee Gaing, Particle swarm optimization to solving
the economic dispatch considering the generator constraints, IEEE
Transactions on \textbf{18} Power Systems, pp. 1187-1195, 2003.

\bibitem{go_econ2}M. Basu, A simulated annealing-based goal-attainment
method for economic emission load dispatch of fixed head hydrothermal
power systems, International Journal of Electrical Power \& Energy
Systems \textbf{27}, pp. 147-153, 2005.

\bibitem{go_med1}Y. Cherruault, Global optimization in biology and
medicine, Mathematical and Computer Modelling \textbf{20}, pp. 119-132,
1994.

\bibitem{go_med2}Eva K. Lee, Large-Scale Optimization-Based Classification
Models in Medicine and Biology, Annals of Biomedical Engineering \textbf{35},
pp 1095-1109, 2007.

\bibitem{crs1}W. L. Price, Global optimization by controlled random
search, Journal of Optimization Theory and Applications \textbf{40},
pp. 333-348, 1983.

\bibitem{crs2}Ivan K\v{r}ivý, Josef Tvrdík, The controlled random
search algorithm in optimizing regression models, Computational Statistics
\& Data Analysis \textbf{20}, pp. 229-234, 1995.

\bibitem{crs3}M.M. Ali, A. Törn, and S. Viitanen, A Numerical Comparison
of Some Modified Controlled Random Search Algorithms, Journal of Global
Optimization \textbf{11},pp. 377--385,1997.

\bibitem{simann_major}S. Kirkpatrick, CD Gelatt, , MP Vecchi, Optimization
by simulated annealing, Science \textbf{220}, pp. 671-680, 1983.

\bibitem{simann1}L. Ingber, Very fast simulated re-annealing, Mathematical
and Computer Modelling \textbf{12}, pp. 967-973, 1989.

\bibitem{simann2}R.W. Eglese, Simulated annealing: A tool for operational
research, Simulated annealing: A tool for operational research \textbf{46},
pp. 271-281, 1990.

\bibitem{diffe1}R. Storn, K. Price, Differential Evolution - A Simple
and Efficient Heuristic for Global Optimization over Continuous Spaces,
Journal of Global Optimization \textbf{11}, pp. 341-359, 1997.

\bibitem{diffe2}J. Liu, J. Lampinen, A Fuzzy Adaptive Differential
Evolution Algorithm. Soft Comput \textbf{9}, pp.448--462, 2005.

\bibitem{pso_major}J. Kennedy and R. Eberhart, \textquotedbl Particle
swarm optimization,\textquotedbl{} Proceedings of ICNN'95 - International
Conference on Neural Networks, 1995, pp. 1942-1948 vol.4, doi: 10.1109/ICNN.1995.488968.

\bibitem{pso1}Riccardo Poli, James Kennedy kennedy, Tim Blackwell,
Particle swarm optimization An Overview, Swarm Intelligence \textbf{1},
pp 33-57, 2007. 

\bibitem{pso2}Ioan Cristian Trelea, The particle swarm optimization
algorithm: convergence analysis and parameter selection, Information
Processing Letters \textbf{85}, pp. 317-325, 2003.

\bibitem{aco1}M. Dorigo, M. Birattari and T. Stutzle, Ant colony
optimization, IEEE Computational Intelligence Magazine \textbf{1},
pp. 28-39, 2006.

\bibitem{aco2}K. Socha, M. Dorigo, Ant colony optimization for continuous
domains, European Journal of Operational Research 185, pp. 1155-1173,
2008.

\bibitem{ga1}D. Goldberg, Genetic Algorithms in Search, Optimization
and Machine Learning, Addison-Wesley Publishing Company, Reading,
Massachussets, 1989.

\bibitem{ga2}Z. Michaelewicz, Genetic Algorithms + Data Structures
= Evolution Programs. Springer - Verlag, Berlin, 1996.

\bibitem{ga3}S.A. Grady, M.Y. Hussaini, M.M. Abdullah, Placement
of wind turbines using genetic algorithms, Renewable Energy \textbf{30},
pp. 259-270, 2005.

\bibitem{FloudasReview}C.A. Floudas, C.E. Gounaris, A review of recent
advances in global optimization, J Glob Optim \textbf{45}, pp. 3--38,
2009.

\bibitem{pso_hybrid1}Y. Da, G. Xiurun, An improved PSO-based ANN
with simulated annealing technique, Neurocomputing \textbf{63}, pp.
527-533, 2005.

\bibitem{pso_hybrid2}H. Liu, Z. Cai, Y. Wang, Hybridizing particle
swarm optimization with differential evolution for constrained numerical
and engineering optimization, Applied Soft Computing \textbf{10},
pp. 629-640, 2010.

\bibitem{pso_hybrid3}X. Pan, L. Xue, Y. Lu Y. et al, Hybrid particle
swarm optimization with simulated annealing, Multimed Tools Appl \textbf{78},
pp. 29921--29936, 2019.

\bibitem{tmlsl}M.M. Ali, C. Storey, Topographical multilevel single
linkage, J. Global Optimization \textbf{5}, pp. 349--358,1994

\bibitem{Salhi}S. Salhi, N.M. Queen, A hybrid algorithm for identifying
global and local minima when optimizing functions with many minima,
European J. Oper. Res. \textbf{155}, pp. 51--67, 2004.

\bibitem{minfinder}I. G. Tsoulos and I. E. Lagaris, MinFinder: Locating
all the local minima of a function, Computer Physics Communications
\textbf{174}, pp. 166-179, 2006. 

\bibitem{msstop1}B. Betro`, F. Schoen, Optimal and sub-optimal stopping
rules for the multistart algorithm in global optimization, Math. Program.
\textbf{57}, pp. 445--458, 1992.

\bibitem{msstop2}W.E. Hart, Sequential stopping rules for random
optimization methods with applications to multistart local search,
Siam J. Optim. \textbf{9}, pp. 270--290, 1998.

\bibitem{msstop3}I.E. Lagaris and I.G. Tsoulos, Stopping Rules for
Box-Constrained Stochastic Global Optimization, Applied Mathematics
and Computation \textbf{197}, pp. 622-632, 2008.

\bibitem{parallel-pso}J. F. Schutte, J. A. Reinbolt, B. J. Fregly, R.
T. Haftka, A. D. George, Parallel global optimization with the particle
swarm algorithm, International journal for Numerical methods in Engineering
\textbf{61}, pp. 2296-2315, 2004.

\bibitem{parallel-multistart}J. Larson and S.M. Wild, Asynchronously
parallel optimization solver for finding multiple minima, Mathematical
Programming Computation \textbf{10}, pp. 303-332, 2018.

\bibitem{parallel-doublepop}I.G. Tsoulos, A. Tzallas, D. Tsalikakis,
PDoublePop: An implementation of parallel genetic algorithm for function
optimization, Computer Physics Communications \textbf{209}, pp. 183-189,
2016.

\bibitem{msgpu1}R. Kamil, S. Reiji, An Efficient GPU Implementation
of a Multi-Start TSP Solver for Large Problem Instances, Proceedings
of the 14th Annual Conference Companion on Genetic and Evolutionary
Computation, pp. 1441-1442, 2012.

\bibitem{msgpu2}Van Luong T., Melab N., Talbi EG. (2011) GPU-Based
Multi-start Local Search Algorithms. In: Coello C.A.C. (eds) Learning
and Intelligent Optimization. LION 2011. Lecture Notes in Computer
Science, vol 6683. Springer, Berlin, Heidelberg. 

\bibitem{qana}Z. Hoda, M.H. Nadimi-Shahraki, A. H. Gandomi, QANA:
Quantum-based avian navigation optimizer algorithm, Engineering Applications
of Artificial Intelligence \textbf{104}, 104314, 2021.

\bibitem{tsa}F.S. Gharehchopogh, An Improved Tunicate Swarm Algorithm
with Best-random Mutation Strategy for Global Optimization Problems,
J Bionic Eng \textbf{19}, pp. 1177--1202, 2022.

\bibitem{starling1}Z. Hoda, M.H. Nadimi-Shahraki, A.H. Gandomi, Starling
murmuration optimizer: A novel bio-inspired algorithm for global and
engineering optimization, Computer Methods in Applied Mechanics and
Engineering \textbf{392}, 114616, 2022.

\bibitem{starling2}M.H. Nadimi-Shahraki et al, Binary Starling Murmuration
Optimizer Algorithm to Select Effective Features from Medical Data,
Applied Sciences \textbf{13.1}, 564, 2023.

\bibitem{dmde}M.H. Nadimi-Shahraki, Z. Hoda, DMDE: Diversity-maintained
multi-trial vector differential evolution algorithm for non-decomposition
large-scale global optimization, Expert Systems with Applications
\textbf{198}, 116895, 2022.

\bibitem{flame}M.H. Nadimi-Shahraki et al., An improved moth-flame
optimization algorithm with adaptation mechanism to solve numerical
and mechanical engineering problems, Entropy \textbf{23.12}, 1637,
2021.

\bibitem{dwarf}J.O. Agushaka, A.E. Ezugwu, L. Abualigah, Dwarf mongoose
optimization algorithm, Computer methods in applied mechanics and
engineering \textbf{391},114570, 2022.

\bibitem{multistart-tsp1}Li W., A Parallel Multi-start Search Algorithm
for Dynamic Traveling Salesman Problem. In: Pardalos P.M., Rebennack
S. (eds) Experimental Algorithms. SEA 2011. Lecture Notes in Computer
Science, vol 6630. Springer, Berlin, Heidelberg, 2011.

\bibitem{multistart_tsp2}R. Martí, M.G.C. Resende, C.C. Ribeiro,
Multi-start methods for combinatorial optimization, European Journal
of Operational Research \textbf{226}, pp. 1-8, 2013.

\bibitem{multistart_tsp3}V. Pandiri, A. Singh, Two multi-start heuristics
for the k-traveling salesman problem, OPSEARCH \textbf{57}, pp. 1164--1204,
2020.

\bibitem{ms_clique1}Q. Wu, J.K. Hao,An adaptive multistart tabu search
approach to solve the maximum clique problem, J Comb Optim \textbf{26},
pp.86--108, 2013.

\bibitem{ms_clique2}Y. Djeddi, H.A. Haddadene, N. Belacel, An extension
of adaptive multi-start tabu search for the maximum quasi-clique problem,
Computers \& Industrial Engineering \textbf{132}, pp. 280-292, 2019.

\bibitem{multistart-vehicle1}Olli Bräysy, Geir Hasle, Wout Dullaert,
A multi-start local search algorithm for the vehicle routing problem
with time windows, European Journal of Operational Research \textbf{159},
pp. 586-605, 2004.

\bibitem{multistart-vehicle2}J. Michallet, C. Prins, L. Amodeo, F.
Yalaoui, G. Vitry, Multi-start iterated local search for the periodic
vehicle routing problem with time windows and time spread constraints
on services, Computers \& Operations Research \textbf{41}, pp. 196-207,
2014.

\bibitem{ms_schedule1}K. Peng, Q.K. Pan, L. Gao, X. Li, S. Das, B.
Zhang, A multi-start variable neighbourhood descent algorithm for
hybrid flowshop rescheduling, Swarm and Evolutionary Computation \textbf{45},
pp. 92-112, 2019.

\bibitem{ms_schedule2}J.Y. Mao, Q.K. Pan, Z.H. Miao, L. Gao, An effective
multi-start iterated greedy algorithm to minimize makespan for the
distributed permutation flowshop scheduling problem with preventive
maintenance, Expert Systems with Applications \textbf{169}, 114495,
2021.

\bibitem{rbf_main1}J. Park, I.W. Sandberg, Approximation and Radial-Basis-Function
Networks, Neural Computation \textbf{5}, pp. 305-316, 1993.

\bibitem{rbf_face}S.H. Yoo, S.K. Oh, W. Pedrycz, Optimized face recognition
algorithm using radial basis function neural networks and its practical
applications, Neural Networks\textbf{ 69, }pp. 111-125, 2015.

\bibitem{rbf_function1}G.B. Huang, P. Saratchandran, N. Sundararajan,
A generalized growing and pruning RBF (GGAP-RBF) neural network for
function approximation, IEEE Transactions on Neural Networks \textbf{16},
pp. 57-67, 2005.

\bibitem{rbf_function2}Z. Majdisova, V. Skala, Radial basis function
approximations: comparison and applications, Applied Mathematical
Modelling \textbf{51}, pp. 728-743, 2017.

\bibitem{rbf_image}B.C. Kuo, H.H. Ho, C. H. Li, C. C. Hung , J. S.
Taur, A Kernel-Based Feature Selection Method for SVM With RBF Kernel
for Hyperspectral Image Classification, IEEE Journal of Selected Topics
in Applied Earth Observations and Remote Sensing. \textbf{7}, pp.
317-326, 2014.

\bibitem{Tsoulos1}I.G. Tsoulos, Modifications of real code genetic
algorithm for global optimization, Applied Mathematics and Computation
\textbf{203}, pp. 598-607, 2008.

\bibitem{powell}M.J.D Powell, A Tolerant Algorithm for Linearly Constrained
Optimization Calculations, Mathematical Programming \textbf{45}, pp.
547-566, 1989. 

\bibitem{kmeans}MacQueen, J.: Some methods for classification and
analysis of multivariate observations, in: Proceedings of the fifth
Berkeley symposium on mathematical statistics and probability \textbf{1},
pp. 281-297, 1967. 

\bibitem{Ali1}M. Montaz Ali, Charoenchai Khompatraporn, Zelda B.
Zabinsky, A Numerical Evaluation of Several Stochastic Algorithms
on Selected Continuous Global Optimization Test Problems, Journal
of Global Optimization \textbf{31}, pp 635-672, 2005. 

\bibitem{Floudas1}C.A. Floudas, P.M. Pardalos, C. Adjiman, W. Esposoto,
Z. G$\ddot{\mbox{u}}$m$\ddot{\mbox{u}}$s, S. Harding, J. Klepeis,
C. Meyer, C. Schweiger, Handbook of Test Problems in Local and Global
Optimization, Kluwer Academic Publishers, Dordrecht, 1999.

\bibitem{kaelo}P. Kaelo, M.M. Ali, Integrated crossover rules in
real coded genetic algorithms, European Journal of Operational Research
\textbf{176}, pp. 60-76, 2007.

\bibitem{Jones}J.E. Lennard-Jones, On the Determination of Molecular
Fields, Proc. R. Soc. Lond. A \textbf{ 106}, pp. 463--477, 1924.
\end{thebibliography}

\end{document}
